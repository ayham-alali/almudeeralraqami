"""
Al-Mudeer Advanced Message Analysis
Enhanced entity extraction, intent detection, and NLP for Arabic business context
"""

import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta


@dataclass
class AnalysisResult:
    """Comprehensive message analysis result"""
    
    # Intent & classification
    primary_intent: str
    secondary_intent: Optional[str]
    intent_confidence: float  # 0.0 - 1.0
    intent_signals: List[str]  # Why we detected this intent
    
    # Urgency
    urgency_level: str  # critical, high, normal, low
    urgency_score: int  # 1-10
    urgency_signals: List[str]
    has_deadline: bool
    deadline_text: Optional[str]
    
    # Sentiment
    sentiment: str  # positive, neutral, negative
    sentiment_score: float  # -1.0 to 1.0
    emotional_cues: List[str]
    frustration_level: int  # 0-10
    
    # Language
    language: str
    dialect: str
    formality_level: str  # formal, semi-formal, informal
    
    # Entities
    entities: Dict[str, Any]
    
    # Summary
    key_points: List[str]
    action_items: List[str]
    questions_asked: List[str]
    
    # Metadata
    word_count: int
    has_attachments_mentioned: bool
    is_reply: bool
    is_forwarded: bool


# ============ Intent Detection ============

INTENT_PATTERNS = {
    "Ø§Ø³ØªÙØ³Ø§Ø±_Ø³Ø¹Ø±": {
        "patterns": ["ÙƒÙ… Ø³Ø¹Ø±", "Ø§Ù„Ø³Ø¹Ø±", "ØªÙƒÙ„ÙØ©", "Ø£Ø³Ø¹Ø§Ø±", "ÙƒÙ… ÙŠÙƒÙ„Ù", "Ø¨ÙƒÙ…"],
        "weight": 1.0,
        "ar": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„Ø³Ø¹Ø±",
    },
    "Ø§Ø³ØªÙØ³Ø§Ø±_ØªÙˆÙØ±": {
        "patterns": ["Ù…ØªÙˆÙØ±", "Ø¹Ù†Ø¯ÙƒÙ…", "ÙŠÙˆØ¬Ø¯", "Ù…ØªØ§Ø­", "Ù…ÙˆØ¬ÙˆØ¯", "ÙÙŠ Ø³ØªÙˆÙƒ"],
        "weight": 0.9,
        "ar": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø§Ù„ØªÙˆÙØ±",
    },
    "Ø§Ø³ØªÙØ³Ø§Ø±_Ø¹Ø§Ù…": {
        "patterns": ["ÙƒÙŠÙ", "Ù…Ø§ Ù‡ÙŠ", "Ù…Ø§ Ù‡Ùˆ", "Ù…Ù…ÙƒÙ† Ø£Ø¹Ø±Ù", "Ø³Ø¤Ø§Ù„", "Ø§Ø³ØªÙØ³Ø§Ø±"],
        "weight": 0.7,
        "ar": "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ø§Ù…",
    },
    "Ø·Ù„Ø¨_Ø®Ø¯Ù…Ø©": {
        "patterns": ["Ø£Ø±ÙŠØ¯", "Ø£Ø±ØºØ¨", "Ø£Ø­ØªØ§Ø¬", "Ø£Ø¨ØºÙ‰", "Ø¨Ø¯ÙŠ", "Ù†Ø±ÙŠØ¯", "Ø·Ù„Ø¨"],
        "weight": 1.0,
        "ar": "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©",
    },
    "Ø·Ù„Ø¨_Ù…ÙˆØ¹Ø¯": {
        "patterns": ["Ù…ÙˆØ¹Ø¯", "Ø­Ø¬Ø²", "Ù…ÙŠØ¹Ø§Ø¯", "Ø£Ø­Ø¬Ø²", "ÙˆÙ‚Øª Ù…Ù†Ø§Ø³Ø¨"],
        "weight": 0.95,
        "ar": "Ø·Ù„Ø¨ Ù…ÙˆØ¹Ø¯",
    },
    "Ø´ÙƒÙˆÙ‰": {
        "patterns": ["Ø´ÙƒÙˆÙ‰", "Ù…Ø´ÙƒÙ„Ø©", "Ù„Ù… ÙŠØ¹Ù…Ù„", "Ù„Ø§ ÙŠØ¹Ù…Ù„", "ØªØ£Ø®Ø±", "Ø³ÙŠØ¡", "Ø®Ø±Ø¨Ø§Ù†"],
        "weight": 1.0,
        "ar": "Ø´ÙƒÙˆÙ‰",
    },
    "Ø´ÙƒÙˆÙ‰_Ø®Ø¯Ù…Ø©": {
        "patterns": ["Ø®Ø¯Ù…Ø© Ø³ÙŠØ¦Ø©", "Ù…Ø¹Ø§Ù…Ù„Ø©", "Ù„Ù… ÙŠØ±Ø¯", "ØªØ¬Ø§Ù‡Ù„", "Ù…Ø§ ÙÙŠ Ø±Ø¯"],
        "weight": 0.9,
        "ar": "Ø´ÙƒÙˆÙ‰ Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø©",
    },
    "Ø´ÙƒÙˆÙ‰_Ù…Ù†ØªØ¬": {
        "patterns": ["Ù…Ù†ØªØ¬ Ù…Ø¹ÙŠØ¨", "Ù…ÙƒØ³ÙˆØ±", "ØºÙ„Ø·", "Ù†Ø§Ù‚Øµ", "ØªØ§Ù„Ù"],
        "weight": 0.9,
        "ar": "Ø´ÙƒÙˆÙ‰ Ù…Ù† Ø§Ù„Ù…Ù†ØªØ¬",
    },
    "Ù…ØªØ§Ø¨Ø¹Ø©": {
        "patterns": ["Ù…ØªØ§Ø¨Ø¹Ø©", "Ø¨Ø®ØµÙˆØµ", "Ø§Ø³ØªÙƒÙ…Ø§Ù„", "ØªØ°ÙƒÙŠØ±", "Ø´Ùˆ ØµØ§Ø±", "ÙˆÙŠÙ† ØµØ§Ø±"],
        "weight": 1.0,
        "ar": "Ù…ØªØ§Ø¨Ø¹Ø© Ø·Ù„Ø¨",
    },
    "Ù…ØªØ§Ø¨Ø¹Ø©_Ø·Ù„Ø¨": {
        "patterns": ["Ø±Ù‚Ù… Ø§Ù„Ø·Ù„Ø¨", "Ø·Ù„Ø¨ÙŠ", "Ø£ÙŠÙ† Ø·Ù„Ø¨ÙŠ", "ÙˆØµÙ„ Ø§Ù„Ø·Ù„Ø¨"],
        "weight": 0.95,
        "ar": "Ù…ØªØ§Ø¨Ø¹Ø© Ø·Ù„Ø¨ Ù…Ø­Ø¯Ø¯",
    },
    "Ø¹Ø±Ø¶_Ø´Ø±Ø§ÙƒØ©": {
        "patterns": ["Ø¹Ø±Ø¶", "Ø´Ø±Ø§ÙƒØ©", "ØªØ¹Ø§ÙˆÙ†", "Ø§ØªÙØ§Ù‚ÙŠØ©", "ÙˆÙƒØ§Ù„Ø©"],
        "weight": 0.8,
        "ar": "Ø¹Ø±Ø¶ Ø´Ø±Ø§ÙƒØ©",
    },
    "Ø¹Ø±Ø¶_ØªØ³ÙˆÙŠÙ‚": {
        "patterns": ["Ø¥Ø¹Ù„Ø§Ù†", "ØªØ³ÙˆÙŠÙ‚", "ØªØ±ÙˆÙŠØ¬", "Ø­Ù…Ù„Ø©"],
        "weight": 0.7,
        "ar": "Ø¹Ø±Ø¶ ØªØ³ÙˆÙŠÙ‚ÙŠ",
    },
    "Ø´ÙƒØ±_Ø±Ø¶Ø§": {
        "patterns": ["Ø´ÙƒØ±Ø§Ù‹", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "Ù…Ø´ÙƒÙˆØ±ÙŠÙ†", "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒÙ… Ø§Ù„Ø¹Ø§ÙÙŠØ©"],
        "weight": 0.8,
        "ar": "Ø´ÙƒØ± ÙˆØªÙ‚Ø¯ÙŠØ±",
    },
    "Ø±Ø¯ÙˆØ¯_ØªÙ„Ù‚Ø§Ø¦ÙŠØ©": {
        "patterns": ["Ø®Ø§Ø±Ø¬ Ø§Ù„Ù…ÙƒØªØ¨", "Ø¥Ø¬Ø§Ø²Ø©", "auto-reply", "automatic"],
        "weight": 0.9,
        "ar": "Ø±Ø¯ ØªÙ„Ù‚Ø§Ø¦ÙŠ",
    },
    "Ø·Ù„Ø¨_Ø¥Ù„ØºØ§Ø¡": {
        "patterns": ["Ø¥Ù„ØºØ§Ø¡", "Ø£Ù„ØºÙŠ", "Ù„Ø§ Ø£Ø±ÙŠØ¯", "ØªØ±Ø§Ø¬Ø¹", "Ø±Ø¬ÙˆØ¹"],
        "weight": 1.0,
        "ar": "Ø·Ù„Ø¨ Ø¥Ù„ØºØ§Ø¡",
    },
    "Ø·Ù„Ø¨_Ø§Ø³ØªØ±Ø¯Ø§Ø¯": {
        "patterns": ["Ø§Ø³ØªØ±Ø¯Ø§Ø¯", "Ø§Ø±Ø¬Ø§Ø¹", "ÙÙ„ÙˆØ³ÙŠ", "Ø§Ù„Ù…Ø¨Ù„Øº", "refund"],
        "weight": 1.0,
        "ar": "Ø·Ù„Ø¨ Ø§Ø³ØªØ±Ø¯Ø§Ø¯",
    },
}


def detect_intent(message: str) -> Tuple[str, Optional[str], float, List[str]]:
    """
    Detect primary and secondary intent with confidence score.
    Returns: (primary_intent, secondary_intent, confidence, signals)
    """
    scores = {}
    signals = {}
    
    message_lower = message.lower()
    
    for intent_key, intent_data in INTENT_PATTERNS.items():
        score = 0
        found_patterns = []
        
        for pattern in intent_data["patterns"]:
            if pattern in message_lower or pattern in message:
                score += intent_data["weight"]
                found_patterns.append(pattern)
        
        if score > 0:
            scores[intent_key] = score
            signals[intent_key] = found_patterns
    
    if not scores:
        return "Ø£Ø®Ø±Ù‰", None, 0.5, ["Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù†Ù…Ø· ÙˆØ§Ø¶Ø­"]
    
    # Sort by score
    sorted_intents = sorted(scores.items(), key=lambda x: -x[1])
    
    primary = sorted_intents[0][0]
    primary_score = sorted_intents[0][1]
    
    # Map to general category
    general_intent = primary.split("_")[0] if "_" in primary else primary
    
    secondary = None
    if len(sorted_intents) > 1:
        secondary = sorted_intents[1][0]
    
    # Calculate confidence
    max_possible = max(d["weight"] * len(d["patterns"]) for d in INTENT_PATTERNS.values())
    confidence = min(1.0, primary_score / (max_possible * 0.3))
    
    return general_intent, secondary, round(confidence, 2), signals.get(primary, [])


# ============ Urgency Detection ============

URGENCY_SIGNALS = {
    "critical": {
        "patterns": ["Ø·Ø§Ø±Ø¦", "ÙÙˆØ±Ø§Ù‹", "Ø§Ù„Ø¢Ù†", "Ø­Ø§Ù„Ø§Ù‹", "Ù‚Ø¨Ù„ ÙÙˆØ§Øª Ø§Ù„Ø£ÙˆØ§Ù†", "Ù…Ø³ØªØ¹Ø¬Ù„ Ø¬Ø¯Ø§Ù‹"],
        "score": 10,
    },
    "high": {
        "patterns": ["Ø¹Ø§Ø¬Ù„", "Ø¶Ø±ÙˆØ±ÙŠ", "Ø§Ù„ÙŠÙˆÙ…", "Ø¨Ø£Ø³Ø±Ø¹ ÙˆÙ‚Øª", "Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹", "Ù„Ø§Ø²Ù…"],
        "score": 8,
    },
    "normal": {
        "patterns": ["Ù…ØªÙ‰ Ù…Ø§ Ù…Ù…ÙƒÙ†", "Ù‚Ø±ÙŠØ¨Ø§Ù‹", "Ù„Ùˆ Ø³Ù…Ø­Øª", "Ø¥Ø°Ø§ Ù…Ù…ÙƒÙ†"],
        "score": 5,
    },
    "low": {
        "patterns": ["Ù„Ø§Ø­Ù‚Ø§Ù‹", "Ø¹Ù†Ø¯Ù…Ø§ ØªØªÙˆÙØ±", "Ù…Ùˆ Ù…Ø³ØªØ¹Ø¬Ù„", "Ù…ØªÙ‰ Ù…Ø§ ØªÙ‚Ø¯Ø±", "Ø¨ÙˆÙ‚Øª ÙØ±Ø§ØºÙƒ"],
        "score": 2,
    },
}

DEADLINE_PATTERNS = [
    r'(?:Ù‚Ø¨Ù„|Ø­ØªÙ‰|Ø¨Ø­Ù„ÙˆÙ„)\s+(?:ÙŠÙˆÙ…\s+)?(\d{1,2}[/\-]\d{1,2}(?:[/\-]\d{2,4})?)',
    r'(?:Ù‚Ø¨Ù„|Ø®Ù„Ø§Ù„)\s+(\d+)\s*(?:ÙŠÙˆÙ…|Ø³Ø§Ø¹Ø©|Ø£Ø³Ø¨ÙˆØ¹)',
    r'(?:ÙŠÙˆÙ…\s+)?(?:Ø§Ù„Ø£Ø­Ø¯|Ø§Ù„Ø§Ø«Ù†ÙŠÙ†|Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡|Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡|Ø§Ù„Ø®Ù…ÙŠØ³|Ø§Ù„Ø¬Ù…Ø¹Ø©|Ø§Ù„Ø³Ø¨Øª)',
    r'(?:ØºØ¯Ø§Ù‹|Ø¨ÙƒØ±Ø©|Ø¨Ø¹Ø¯ ØºØ¯|Ø§Ù„ÙŠÙˆÙ…)',
]


def detect_urgency(message: str) -> Tuple[str, int, List[str], bool, Optional[str]]:
    """
    Detect urgency level with signals and deadline.
    Returns: (level, score, signals, has_deadline, deadline_text)
    """
    found_signals = []
    max_score = 5  # Default normal
    level = "normal"
    
    message_lower = message.lower()
    
    for urgency_level, data in URGENCY_SIGNALS.items():
        for pattern in data["patterns"]:
            if pattern in message_lower or pattern in message:
                found_signals.append(pattern)
                if data["score"] > max_score:
                    max_score = data["score"]
                    level = urgency_level
    
    # Check for deadlines
    has_deadline = False
    deadline_text = None
    
    for pattern in DEADLINE_PATTERNS:
        match = re.search(pattern, message)
        if match:
            has_deadline = True
            deadline_text = match.group(0)
            if max_score < 7:
                max_score = 7
                level = "high"
            break
    
    # Exclamation marks increase urgency
    exclamation_count = message.count("!")
    if exclamation_count >= 3:
        max_score = min(10, max_score + 1)
        found_signals.append(f"Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¹Ø¬Ø¨ ({exclamation_count})")
    
    # ALL CAPS increases urgency
    upper_ratio = sum(1 for c in message if c.isupper()) / max(len(message), 1)
    if upper_ratio > 0.5:
        max_score = min(10, max_score + 1)
        found_signals.append("Ø£Ø­Ø±Ù ÙƒØ¨ÙŠØ±Ø© (ØµØ±Ø§Ø®)")
    
    return level, max_score, found_signals, has_deadline, deadline_text


# ============ Sentiment Analysis ============

SENTIMENT_PATTERNS = {
    "positive": {
        "strong": ["Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "Ù…Ø°Ù‡Ù„", "Ø£ÙØ¶Ù„", "Ø³Ø¹ÙŠØ¯ Ø¬Ø¯Ø§Ù‹", "â¤ï¸", "ğŸ‘", "ğŸ‰"],
        "mild": ["Ø´ÙƒØ±Ø§Ù‹", "Ø¬ÙŠØ¯", "Ø­Ù„Ùˆ", "ØªÙ…Ø§Ù…", "Ù…Ø³Ø±ÙˆØ±", "Ø±Ø§Ø¶ÙŠ", "ğŸ‘", "ğŸ˜Š"],
    },
    "negative": {
        "strong": ["Ø³ÙŠØ¡ Ø¬Ø¯Ø§Ù‹", "Ø£Ø³ÙˆØ£", "ÙƒØ§Ø±Ø«Ø©", "Ù…Ø­Ø¨Ø·", "ØºØ§Ø¶Ø¨", "Ù…Ø³ØªØ§Ø¡ Ø¬Ø¯Ø§Ù‹", "ğŸ˜¡", "ğŸ’¢"],
        "mild": ["Ù„Ù„Ø£Ø³Ù", "Ù…Ø´ÙƒÙ„Ø©", "ØµØ¹Ø¨", "Ù…ØªØ£Ø®Ø±", "ØºÙŠØ± Ø±Ø§Ø¶ÙŠ", "ğŸ˜”", "ğŸ˜"],
    },
}

FRUSTRATION_SIGNALS = [
    "ÙƒÙ… Ù…Ø±Ø©", "Ù…Ø±Ø© Ø«Ø§Ù†ÙŠØ©", "Ù…Ø±Ø© Ø£Ø®Ø±Ù‰", "Ù„Ù… Ø£Ø­ØµÙ„", "Ù„Ø§ Ø¬ÙˆØ§Ø¨", "Ù„Ø§ Ø±Ø¯",
    "Ø§Ù†ØªØ¸Ø±Øª", "Ø£Ù†ØªØ¸Ø± Ù…Ù†Ø°", "Ù…Ù† Ø²Ù…Ø§Ù†", "Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†", "Ù„Ø­Ø¯ Ø§Ù„Ø¢Ù†",
]


def detect_sentiment(message: str) -> Tuple[str, float, List[str], int]:
    """
    Detect sentiment with score and emotional cues.
    Returns: (sentiment, score, cues, frustration_level)
    """
    positive_score = 0
    negative_score = 0
    cues = []
    
    for pattern in SENTIMENT_PATTERNS["positive"]["strong"]:
        if pattern in message:
            positive_score += 2
            cues.append(f"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ù‚ÙˆÙŠ: {pattern}")
    
    for pattern in SENTIMENT_PATTERNS["positive"]["mild"]:
        if pattern in message:
            positive_score += 1
            cues.append(f"Ø¥ÙŠØ¬Ø§Ø¨ÙŠ: {pattern}")
    
    for pattern in SENTIMENT_PATTERNS["negative"]["strong"]:
        if pattern in message:
            negative_score += 2
            cues.append(f"Ø³Ù„Ø¨ÙŠ Ù‚ÙˆÙŠ: {pattern}")
    
    for pattern in SENTIMENT_PATTERNS["negative"]["mild"]:
        if pattern in message:
            negative_score += 1
            cues.append(f"Ø³Ù„Ø¨ÙŠ: {pattern}")
    
    # Calculate frustration
    frustration = 0
    for signal in FRUSTRATION_SIGNALS:
        if signal in message:
            frustration += 2
            cues.append(f"Ø¥Ø­Ø¨Ø§Ø·: {signal}")
    
    frustration = min(10, frustration)
    
    # Calculate final score (-1.0 to 1.0)
    total = positive_score + negative_score
    if total == 0:
        score = 0.0
        sentiment = "Ù…Ø­Ø§ÙŠØ¯"
    else:
        score = (positive_score - negative_score) / max(total, 1)
        score = max(-1.0, min(1.0, score))
        
        if score > 0.3:
            sentiment = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
        elif score < -0.3:
            sentiment = "Ø³Ù„Ø¨ÙŠ"
        else:
            sentiment = "Ù…Ø­Ø§ÙŠØ¯"
    
    # Frustration affects sentiment
    if frustration >= 5 and sentiment != "Ø³Ù„Ø¨ÙŠ":
        sentiment = "Ø³Ù„Ø¨ÙŠ"
        score = min(score, -0.3)
    
    return sentiment, round(score, 2), cues, frustration


# ============ Entity Extraction ============

ENTITY_PATTERNS = {
    "phone_syria": r'(?:00963|\+963|0)?9\d{8}',
    "phone_saudi": r'(?:00966|\+966|0)?5\d{8}',
    "phone_uae": r'(?:00971|\+971|0)?5\d{8}',
    "phone_general": r'\+?\d{10,15}',
    "email": r'[\w\.-]+@[\w\.-]+\.\w+',
    "date": r'\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4}',
    "time": r'\d{1,2}:\d{2}(?:\s*[ØµÙ…])?',
    "money_syp": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ù„\.Ø³|Ù„ÙŠØ±Ø© Ø³ÙˆØ±ÙŠØ©|Ù„ÙŠØ±Ø©)',
    "money_sar": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ø±\.Ø³|Ø±ÙŠØ§Ù„|Ø±ÙŠØ§Ù„ Ø³Ø¹ÙˆØ¯ÙŠ)',
    "money_usd": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ø¯ÙˆÙ„Ø§Ø±|\$|USD)',
    "money_aed": r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ø¯Ø±Ù‡Ù…|AED)',
    "order_number": r'(?:Ø·Ù„Ø¨|Ø±Ù‚Ù… Ø§Ù„Ø·Ù„Ø¨|order)[\s#:]*([A-Z0-9\-]{5,20})',
    "invoice_number": r'(?:ÙØ§ØªÙˆØ±Ø©|Ø±Ù‚Ù… Ø§Ù„ÙØ§ØªÙˆØ±Ø©|invoice)[\s#:]*([A-Z0-9\-]{5,20})',
    "url": r'https?://[^\s<>"{}|\\^`\[\]]+',
    "percentage": r'(\d+(?:\.\d+)?)\s*%',
    "quantity": r'(\d+)\s*(?:Ù‚Ø·Ø¹Ø©|Ø­Ø¨Ø©|ÙƒÙŠÙ„Ùˆ|Ø·Ù†|Ù…ØªØ±|Ø¹Ù„Ø¨Ø©|ÙƒØ±ØªÙˆÙ†)',
}

NAME_PATTERNS = [
    r'(?:Ø§Ù„Ø³ÙŠØ¯|Ø§Ù„Ø³ÙŠØ¯Ø©|Ø§Ù„Ø£Ø³ØªØ§Ø°|Ø§Ù„Ø£Ø³ØªØ§Ø°Ø©|Ø§Ù„Ù…Ù‡Ù†Ø¯Ø³|Ø§Ù„Ø¯ÙƒØªÙˆØ±)\s+([\u0600-\u06FF\s]{3,30})',
    r'(?:Ø£Ù†Ø§|Ø§Ø³Ù…ÙŠ|Ø£Ø®ÙˆÙƒ|Ø£Ø®ÙˆÙƒÙ…)\s+([\u0600-\u06FF]{3,20})',
]

LOCATION_PATTERNS = [
    r'(?:Ø§Ù„Ø¹Ù†ÙˆØ§Ù†|Ø§Ù„Ù…ÙˆÙ‚Ø¹|ÙÙŠ|Ø¥Ù„Ù‰)[\s:]+([^ØŒ,\n]{5,50})',
    r'(?:Ø´Ø§Ø±Ø¹|Ø­ÙŠ|Ù…Ù†Ø·Ù‚Ø©|Ù…Ø¯ÙŠÙ†Ø©)\s+([\u0600-\u06FF\s]{3,30})',
]


def extract_entities(message: str) -> Dict[str, Any]:
    """Extract all entities from message"""
    entities = {}
    
    # Extract using patterns
    for entity_type, pattern in ENTITY_PATTERNS.items():
        matches = re.findall(pattern, message, re.IGNORECASE)
        if matches:
            # Clean up matches
            if isinstance(matches[0], tuple):
                matches = [m[0] if isinstance(m, tuple) else m for m in matches]
            entities[entity_type] = list(set(matches))
    
    # Consolidate phone numbers
    phones = []
    for key in list(entities.keys()):
        if key.startswith("phone_"):
            phones.extend(entities.pop(key, []))
    if phones:
        entities["phones"] = list(set(phones))
    
    # Consolidate money
    money = []
    for key in list(entities.keys()):
        if key.startswith("money_"):
            currency = key.split("_")[1].upper()
            for amount in entities.pop(key, []):
                money.append({"amount": amount, "currency": currency})
    if money:
        entities["money"] = money
    
    # Extract names
    for pattern in NAME_PATTERNS:
        match = re.search(pattern, message)
        if match:
            entities["person_name"] = match.group(1).strip()
            break
    
    # Extract locations
    for pattern in LOCATION_PATTERNS:
        match = re.search(pattern, message)
        if match:
            location = match.group(1).strip()
            if len(location) > 5:
                entities["location"] = location
                break
    
    return entities


# ============ Question Detection ============

def extract_questions(message: str) -> List[str]:
    """Extract questions from the message"""
    questions = []
    
    # Split by question marks
    parts = re.split(r'[ØŸ?]', message)
    for part in parts[:-1]:  # Last part won't be a question
        # Get the question part (from last period/newline)
        question = re.split(r'[.\n]', part)[-1].strip()
        if len(question) > 5:
            questions.append(question + "ØŸ")
    
    # Look for question words without question mark
    question_words = ["ÙƒÙŠÙ", "Ù…ØªÙ‰", "Ø£ÙŠÙ†", "Ù„Ù…Ø§Ø°Ø§", "Ù…Ø§ Ù‡ÙŠ", "Ù…Ø§ Ù‡Ùˆ", "Ù‡Ù„", "ÙƒÙ…"]
    sentences = re.split(r'[.\n]', message)
    for sentence in sentences:
        sentence = sentence.strip()
        if any(sentence.startswith(qw) for qw in question_words):
            if sentence not in questions and len(sentence) > 5:
                questions.append(sentence)
    
    return questions[:5]  # Max 5 questions


# ============ Main Analysis Function ============

def analyze_message_advanced(message: str) -> AnalysisResult:
    """
    Perform comprehensive message analysis.
    Returns a detailed AnalysisResult dataclass.
    """
    # Intent detection
    primary_intent, secondary_intent, intent_confidence, intent_signals = detect_intent(message)
    
    # Urgency detection
    urgency_level, urgency_score, urgency_signals, has_deadline, deadline_text = detect_urgency(message)
    
    # Sentiment analysis
    sentiment, sentiment_score, emotional_cues, frustration_level = detect_sentiment(message)
    
    # Entity extraction
    entities = extract_entities(message)
    
    # Question extraction
    questions = extract_questions(message)
    
    # Language detection (simple)
    arabic_ratio = len(re.findall(r'[\u0600-\u06FF]', message)) / max(len(message), 1)
    language = "ar" if arabic_ratio > 0.3 else "en"
    
    # Dialect detection
    dialect = "ÙØµØ­Ù‰"
    dialect_markers = {
        "Ø´Ø§Ù…ÙŠ": ["Ø´Ùˆ", "ÙƒÙŠÙÙƒ", "Ù‡Ù„Ù‚", "Ù„ÙŠÙƒ", "Ù…Ù†ÙŠØ­"],
        "Ø®Ù„ÙŠØ¬ÙŠ": ["ÙˆØ´", "ÙƒØ°Ø§", "Ø²ÙŠÙ†", "ÙˆØ§Ø¬Ø¯"],
        "Ù…ØµØ±ÙŠ": ["Ø¥Ø²ÙŠÙƒ", "ÙƒØ¯Ø©", "Ø®Ø§Ù„Øµ", "Ù‚ÙˆÙŠ"],
    }
    for d, markers in dialect_markers.items():
        if any(m in message for m in markers):
            dialect = d
            break
    
    # Formality detection
    formal_markers = ["Ø§Ù„Ø³ÙŠØ¯", "Ø§Ù„Ù…Ø­ØªØ±Ù…", "Ù†ÙˆØ¯", "ÙŠØ³Ø±Ù†Ø§"]
    informal_markers = ["Ù‡Ø§ÙŠ", "Ù‡Ù„Ø§", "ÙƒÙŠÙÙƒ", "Ø´Ùˆ Ø£Ø®Ø¨Ø§Ø±Ùƒ"]
    
    formal_count = sum(1 for m in formal_markers if m in message)
    informal_count = sum(1 for m in informal_markers if m in message)
    
    if formal_count > informal_count:
        formality = "Ø±Ø³Ù…ÙŠ"
    elif informal_count > formal_count:
        formality = "ØºÙŠØ± Ø±Ø³Ù…ÙŠ"
    else:
        formality = "Ø´Ø¨Ù‡ Ø±Ø³Ù…ÙŠ"
    
    # Key points (first 3 sentences or bullet points)
    key_points = []
    bullets = re.findall(r'[-â€¢*]\s*([^\n]+)', message)
    if bullets:
        key_points = bullets[:3]
    else:
        sentences = re.split(r'[.\n]', message)
        key_points = [s.strip() for s in sentences if len(s.strip()) > 10][:3]
    
    # Action items based on intent
    action_items = []
    if primary_intent == "Ø§Ø³ØªÙØ³Ø§Ø±":
        action_items = ["Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±"]
    elif primary_intent == "Ø·Ù„Ø¨":
        action_items = ["Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨", "ØªØ£ÙƒÙŠØ¯ Ø§Ù„ØªÙØ§ØµÙŠÙ„"]
    elif primary_intent == "Ø´ÙƒÙˆÙ‰":
        action_items = ["ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø´ÙƒÙˆÙ‰", "Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¹Ù…ÙŠÙ„", "Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©"]
    elif primary_intent == "Ù…ØªØ§Ø¨Ø¹Ø©":
        action_items = ["Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ù„Ø©", "Ø¥Ø±Ø³Ø§Ù„ ØªØ­Ø¯ÙŠØ«"]
    
    # Metadata
    word_count = len(message.split())
    has_attachments = any(w in message.lower() for w in ["Ù…Ø±ÙÙ‚", "Ù…Ù„Ù", "ØµÙˆØ±Ø©", "attachment", "attached"])
    is_reply = message.strip().startswith(("Re:", "Ø±Ø¯:", ">>", ">"))
    is_forwarded = any(w in message.lower() for w in ["forwarded", "ØªØ­ÙˆÙŠÙ„", "Fwd:"])
    
    return AnalysisResult(
        primary_intent=primary_intent,
        secondary_intent=secondary_intent,
        intent_confidence=intent_confidence,
        intent_signals=intent_signals,
        urgency_level=urgency_level,
        urgency_score=urgency_score,
        urgency_signals=urgency_signals,
        has_deadline=has_deadline,
        deadline_text=deadline_text,
        sentiment=sentiment,
        sentiment_score=sentiment_score,
        emotional_cues=emotional_cues,
        frustration_level=frustration_level,
        language=language,
        dialect=dialect,
        formality_level=formality,
        entities=entities,
        key_points=key_points,
        action_items=action_items,
        questions_asked=questions,
        word_count=word_count,
        has_attachments_mentioned=has_attachments,
        is_reply=is_reply,
        is_forwarded=is_forwarded,
    )


def analysis_to_dict(result: AnalysisResult) -> Dict[str, Any]:
    """Convert AnalysisResult to dictionary for JSON serialization"""
    return {
        "intent": {
            "primary": result.primary_intent,
            "secondary": result.secondary_intent,
            "confidence": result.intent_confidence,
            "signals": result.intent_signals,
        },
        "urgency": {
            "level": result.urgency_level,
            "score": result.urgency_score,
            "signals": result.urgency_signals,
            "has_deadline": result.has_deadline,
            "deadline": result.deadline_text,
        },
        "sentiment": {
            "label": result.sentiment,
            "score": result.sentiment_score,
            "cues": result.emotional_cues,
            "frustration_level": result.frustration_level,
        },
        "language": {
            "code": result.language,
            "dialect": result.dialect,
            "formality": result.formality_level,
        },
        "entities": result.entities,
        "summary": {
            "key_points": result.key_points,
            "action_items": result.action_items,
            "questions": result.questions_asked,
        },
        "metadata": {
            "word_count": result.word_count,
            "has_attachments": result.has_attachments_mentioned,
            "is_reply": result.is_reply,
            "is_forwarded": result.is_forwarded,
        },
    }
