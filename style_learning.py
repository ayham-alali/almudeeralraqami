"""
Al-Mudeer Style Learning Module
Learn and adapt to user's writing style from their past messages
"""

import json
import re
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import os


@dataclass
class StyleProfile:
    """Learned writing style profile from user's messages"""
    
    # Basic info
    profile_id: str
    license_id: str
    created_at: str
    updated_at: str
    message_count: int  # Number of messages analyzed
    
    # Tone & formality
    formality_level: str  # formal, semi-formal, casual
    warmth_level: str  # warm, neutral, professional
    
    # Language patterns
    primary_language: str  # ar, en, etc.
    dialect: str  # Ø´Ø§Ù…ÙŠØŒ Ø®Ù„ÙŠØ¬ÙŠØŒ Ù…ØµØ±ÙŠØŒ ÙØµØ­Ù‰
    uses_emojis: bool
    emoji_frequency: str  # never, rare, occasional, frequent
    
    # Structure patterns
    avg_response_length: int  # characters
    preferred_length: str  # short, medium, long
    uses_bullet_points: bool
    uses_numbered_lists: bool
    
    # Signature patterns
    common_greetings: List[str]  # Top 3 greetings used
    common_closings: List[str]  # Top 3 closings used
    signature_line: Optional[str]  # If they have a consistent signature
    
    # Phrase patterns
    favorite_phrases: List[str]  # Common phrases they repeat
    transition_words: List[str]  # How they connect ideas
    acknowledgment_style: str  # How they acknowledge receipt
    
    # Personality traits detected
    personality_traits: List[str]  # e.g., "direct", "empathetic", "detailed"
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'StyleProfile':
        return cls(**data)
    
    def to_prompt(self) -> str:
        """Convert profile to prompt instructions for LLM"""
        prompt_parts = []
        
        # Formality
        formality_desc = {
            "formal": "Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø±Ø³Ù…ÙŠØ© ÙˆÙ…Ù‡Ù†ÙŠØ©",
            "semi-formal": "Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø´Ø¨Ù‡ Ø±Ø³Ù…ÙŠØ©ØŒ ÙˆØ¯ÙˆØ¯Ø© Ù„ÙƒÙ† Ù…Ø­ØªØ±Ù…Ø©",
            "casual": "Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹ÙÙˆÙŠØ© ÙˆÙˆØ¯ÙˆØ¯Ø©"
        }
        prompt_parts.append(formality_desc.get(self.formality_level, ""))
        
        # Dialect
        if self.dialect and self.dialect != "ÙØµØ­Ù‰":
            prompt_parts.append(f"Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© {self.dialect} Ø§Ù„Ø®ÙÙŠÙØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©")
        
        # Length
        length_desc = {
            "short": "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ Ù‚ØµÙŠØ±Ø§Ù‹ ÙˆÙ…Ø®ØªØµØ±Ø§Ù‹ (2-3 Ø£Ø³Ø·Ø±)",
            "medium": "Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø·ÙˆÙ„ (4-6 Ø£Ø³Ø·Ø±)",
            "long": "ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù…ÙØµÙ„Ø§Ù‹ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©"
        }
        prompt_parts.append(length_desc.get(self.preferred_length, ""))
        
        # Greetings
        if self.common_greetings:
            prompt_parts.append(f"Ø§Ø³ØªØ®Ø¯Ù… ØªØ­ÙŠØ§Øª Ù…Ø«Ù„: {', '.join(self.common_greetings[:2])}")
        
        # Closings
        if self.common_closings:
            prompt_parts.append(f"Ø§Ø®ØªØªÙ… Ø¨Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„: {', '.join(self.common_closings[:2])}")
        
        # Favorite phrases
        if self.favorite_phrases:
            prompt_parts.append(f"ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„: {', '.join(self.favorite_phrases[:3])}")
        
        # Emojis
        if self.uses_emojis and self.emoji_frequency in ["occasional", "frequent"]:
            prompt_parts.append("ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø®ÙÙŠÙ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© ðŸ‘")
        elif not self.uses_emojis:
            prompt_parts.append("Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø¥ÙŠÙ…ÙˆØ¬ÙŠ")
        
        # Personality
        if self.personality_traits:
            traits_desc = {
                "direct": "ÙƒÙ† Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø±Ø¯ÙˆØ¯",
                "empathetic": "Ø£Ø¸Ù‡Ø± ØªØ¹Ø§Ø·ÙØ§Ù‹ ÙˆØ§Ù‡ØªÙ…Ø§Ù…Ø§Ù‹",
                "detailed": "Ù‚Ø¯Ù… ØªÙØ§ØµÙŠÙ„ ÙˆØ´Ø±Ø­Ø§Ù‹ ÙƒØ§ÙÙŠØ§Ù‹",
                "friendly": "ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ‚Ø±ÙŠØ¨Ø§Ù‹",
                "formal": "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ",
            }
            for trait in self.personality_traits[:2]:
                if trait in traits_desc:
                    prompt_parts.append(traits_desc[trait])
        
        return "\n".join([p for p in prompt_parts if p])


# ============ Analysis Functions ============

async def analyze_messages_for_style(
    messages: List[Dict[str, Any]],
    license_id: str,
) -> StyleProfile:
    """
    Analyze a list of sent messages to extract style patterns.
    
    messages should be a list of dicts with at least:
    - body: str (the message content)
    - channel: str (email, telegram, whatsapp)
    - sent_at: str (timestamp)
    """
    if not messages:
        return create_default_profile(license_id)
    
    # Extract all message bodies
    bodies = [m.get("body", "") for m in messages if m.get("body")]
    
    if len(bodies) < 3:
        # Not enough data
        return create_default_profile(license_id)
    
    # Analyze patterns
    analysis = {
        "formality_level": analyze_formality(bodies),
        "warmth_level": analyze_warmth(bodies),
        "primary_language": detect_primary_language(bodies),
        "dialect": detect_dialect(bodies),
        "uses_emojis": any(has_emojis(b) for b in bodies),
        "emoji_frequency": analyze_emoji_frequency(bodies),
        "avg_response_length": sum(len(b) for b in bodies) // len(bodies),
        "preferred_length": categorize_length(sum(len(b) for b in bodies) // len(bodies)),
        "uses_bullet_points": any("â€¢" in b or "-" in b for b in bodies),
        "uses_numbered_lists": any(re.search(r'\d+[.)]\s', b) for b in bodies),
        "common_greetings": extract_common_greetings(bodies),
        "common_closings": extract_common_closings(bodies),
        "signature_line": extract_signature(bodies),
        "favorite_phrases": extract_favorite_phrases(bodies),
        "transition_words": extract_transition_words(bodies),
        "acknowledgment_style": detect_acknowledgment_style(bodies),
        "personality_traits": detect_personality_traits(bodies),
    }
    
    return StyleProfile(
        profile_id=f"style_{license_id}_{datetime.now().strftime('%Y%m%d')}",
        license_id=license_id,
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat(),
        message_count=len(bodies),
        **analysis
    )


def create_default_profile(license_id: str) -> StyleProfile:
    """Create a default style profile when no data available"""
    return StyleProfile(
        profile_id=f"style_{license_id}_default",
        license_id=license_id,
        created_at=datetime.now().isoformat(),
        updated_at=datetime.now().isoformat(),
        message_count=0,
        formality_level="semi-formal",
        warmth_level="neutral",
        primary_language="ar",
        dialect="ÙØµØ­Ù‰",
        uses_emojis=False,
        emoji_frequency="never",
        avg_response_length=200,
        preferred_length="medium",
        uses_bullet_points=False,
        uses_numbered_lists=False,
        common_greetings=["Ù…Ø±Ø­Ø¨Ø§Ù‹", "Ø£Ù‡Ù„Ø§Ù‹"],
        common_closings=["Ù…Ø¹ Ø§Ù„ØªØ­ÙŠØ©", "Ø´ÙƒØ±Ø§Ù‹"],
        signature_line=None,
        favorite_phrases=[],
        transition_words=["Ø¨Ø®ØµÙˆØµ", "Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€"],
        acknowledgment_style="formal",
        personality_traits=["professional"],
    )


# ============ Analysis Helpers ============

def analyze_formality(texts: List[str]) -> str:
    """Detect formality level from texts"""
    formal_markers = ["Ø§Ù„Ø³ÙŠØ¯", "Ø§Ù„Ø³ÙŠØ¯Ø©", "Ø§Ù„Ù…Ø­ØªØ±Ù…", "Ù†ÙˆØ¯ Ø¥ÙØ§Ø¯ØªÙƒÙ…", "ÙŠØ³Ø±Ù†Ø§"]
    casual_markers = ["Ù‡Ù„Ø§", "ÙƒÙŠÙÙƒ", "Ø´Ùˆ", "ÙˆÙŠÙ†", "Ù„ÙŠØ´", "Ù‡Ø§ÙŠ"]
    
    formal_count = sum(1 for t in texts for m in formal_markers if m in t)
    casual_count = sum(1 for t in texts for m in casual_markers if m in t)
    
    if formal_count > casual_count * 2:
        return "formal"
    elif casual_count > formal_count:
        return "casual"
    return "semi-formal"


def analyze_warmth(texts: List[str]) -> str:
    """Detect warmth level"""
    warm_markers = ["Ø­Ø¨ÙŠØ¨ÙŠ", "Ø¹Ø²ÙŠØ²ÙŠ", "ÙŠØ§ Ø·ÙŠØ¨", "Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©", "â¤ï¸", "ðŸ˜Š"]
    professional_markers = ["ØªØ­ÙŠØ§ØªÙŠ", "Ù…Ø¹ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±", "Ù†Ù‚Ø¯Ø± ØªØ¹Ø§ÙˆÙ†ÙƒÙ…"]
    
    warm_count = sum(1 for t in texts for m in warm_markers if m in t)
    pro_count = sum(1 for t in texts for m in professional_markers if m in t)
    
    if warm_count > len(texts) * 0.3:
        return "warm"
    elif pro_count > warm_count:
        return "professional"
    return "neutral"


def detect_primary_language(texts: List[str]) -> str:
    """Detect primary language"""
    arabic_pattern = re.compile(r'[\u0600-\u06FF]')
    
    arabic_chars = sum(len(arabic_pattern.findall(t)) for t in texts)
    total_chars = sum(len(t) for t in texts)
    
    if total_chars == 0:
        return "ar"
    
    if arabic_chars / total_chars > 0.5:
        return "ar"
    return "en"


def detect_dialect(texts: List[str]) -> str:
    """Detect Arabic dialect"""
    dialect_markers = {
        "Ø´Ø§Ù…ÙŠ": ["Ø´Ùˆ", "ÙƒÙŠÙÙƒ", "Ù‡Ù„Ù‚", "Ù…Ù†ÙŠØ­", "ÙƒØªÙŠØ±", "Ù„ÙŠÙƒ"],
        "Ø®Ù„ÙŠØ¬ÙŠ": ["ÙˆØ´", "ÙƒØ°Ø§", "Ø²ÙŠÙ†", "ÙˆØ§Ø¬Ø¯", "Ø­Ø¨ÙŠØ¨ÙŠ"],
        "Ù…ØµØ±ÙŠ": ["Ø¥Ø²ÙŠÙƒ", "ÙƒØ¯Ø©", "Ø®Ø§Ù„Øµ", "Ù‚ÙˆÙŠ", "ÙŠØ¹Ù†ÙŠ"],
    }
    
    scores = {d: 0 for d in dialect_markers}
    
    for text in texts:
        text_lower = text.lower()
        for dialect, markers in dialect_markers.items():
            for marker in markers:
                if marker in text_lower:
                    scores[dialect] += 1
    
    max_dialect = max(scores, key=scores.get)
    if scores[max_dialect] > len(texts) * 0.1:
        return max_dialect
    return "ÙØµØ­Ù‰"


def has_emojis(text: str) -> bool:
    """Check if text contains emojis"""
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "]+",
        flags=re.UNICODE
    )
    return bool(emoji_pattern.search(text))


def analyze_emoji_frequency(texts: List[str]) -> str:
    """Analyze how often emojis are used"""
    emoji_count = sum(1 for t in texts if has_emojis(t))
    ratio = emoji_count / len(texts) if texts else 0
    
    if ratio == 0:
        return "never"
    elif ratio < 0.1:
        return "rare"
    elif ratio < 0.4:
        return "occasional"
    return "frequent"


def categorize_length(avg_length: int) -> str:
    """Categorize average response length"""
    if avg_length < 150:
        return "short"
    elif avg_length < 400:
        return "medium"
    return "long"


def extract_common_greetings(texts: List[str]) -> List[str]:
    """Extract common greeting patterns"""
    greeting_patterns = [
        r'^(Ù…Ø±Ø­Ø¨Ø§Ù‹?|Ø£Ù‡Ù„Ø§Ù‹?|Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…|Ù‡Ù„Ø§|ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±|Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±)',
        r'^(ØªØ­ÙŠØ© Ø·ÙŠØ¨Ø©|Ø§Ù„Ø³ÙŠØ¯|Ø§Ù„Ø³ÙŠØ¯Ø©)',
    ]
    
    greetings = {}
    for text in texts:
        first_line = text.split('\n')[0][:50]
        for pattern in greeting_patterns:
            match = re.match(pattern, first_line)
            if match:
                greeting = match.group(1)
                greetings[greeting] = greetings.get(greeting, 0) + 1
    
    # Return top 3
    sorted_greetings = sorted(greetings.items(), key=lambda x: -x[1])
    return [g[0] for g in sorted_greetings[:3]]


def extract_common_closings(texts: List[str]) -> List[str]:
    """Extract common closing patterns"""
    closing_patterns = [
        r'(Ù…Ø¹ Ø§Ù„ØªØ­ÙŠØ©|Ø´ÙƒØ±Ø§Ù‹?|ØªØ­ÙŠØ§ØªÙŠ|Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚|Ù…Ø¹ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±)[\s\n]*$',
        r'(Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ† Ù„Ø£ÙŠ Ø³Ø¤Ø§Ù„|ØªÙˆØ§ØµÙ„ Ù…Ø¹Ù†Ø§)[\s\n]*$',
    ]
    
    closings = {}
    for text in texts:
        last_lines = '\n'.join(text.split('\n')[-2:])
        for pattern in closing_patterns:
            match = re.search(pattern, last_lines)
            if match:
                closing = match.group(1)
                closings[closing] = closings.get(closing, 0) + 1
    
    sorted_closings = sorted(closings.items(), key=lambda x: -x[1])
    return [c[0] for c in sorted_closings[:3]]


def extract_signature(texts: List[str]) -> Optional[str]:
    """Extract consistent signature line if present"""
    # Look for lines that appear in many messages at the end
    endings = []
    for text in texts:
        lines = text.strip().split('\n')
        if len(lines) >= 2:
            endings.append(lines[-1].strip())
    
    if not endings:
        return None
    
    # Find most common ending
    from collections import Counter
    counter = Counter(endings)
    most_common = counter.most_common(1)[0]
    
    # If it appears in more than 50% of messages, it's likely a signature
    if most_common[1] > len(texts) * 0.5:
        return most_common[0]
    return None


def extract_favorite_phrases(texts: List[str]) -> List[str]:
    """Extract frequently used phrases"""
    # Common phrase patterns
    phrase_patterns = [
        r'(Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡)',
        r'(Ø§Ù„Ù„Ù‡ ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©)',
        r'(Ù…Ø§ ÙÙŠ Ù…Ø´ÙƒÙ„Ø©)',
        r'(ØªÙ…Ø§Ù…)',
        r'(Ø­Ø§Ø¶Ø±)',
        r'(Ø¨Ø§Ù„Ø¶Ø¨Ø·)',
        r'(Ø·ÙŠØ¨)',
        r'(Ø£ÙƒÙŠØ¯)',
    ]
    
    phrases = {}
    for text in texts:
        for pattern in phrase_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                phrases[match] = phrases.get(match, 0) + 1
    
    sorted_phrases = sorted(phrases.items(), key=lambda x: -x[1])
    return [p[0] for p in sorted_phrases[:5]]


def extract_transition_words(texts: List[str]) -> List[str]:
    """Extract common transition words"""
    transitions = ["Ø¨Ø®ØµÙˆØµ", "Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€", "Ø£Ù…Ø§ Ø¹Ù†", "Ù…Ù† Ù†Ø§Ø­ÙŠØ©", "Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ©"]
    
    found = {}
    for text in texts:
        for trans in transitions:
            if trans in text:
                found[trans] = found.get(trans, 0) + 1
    
    sorted_trans = sorted(found.items(), key=lambda x: -x[1])
    return [t[0] for t in sorted_trans[:3]]


def detect_acknowledgment_style(texts: List[str]) -> str:
    """Detect how they acknowledge receiving messages"""
    formal_ack = ["ØªÙ… Ø§Ø³ØªÙ„Ø§Ù…", "ÙˆØµÙ„ØªÙ†Ø§ Ø±Ø³Ø§Ù„ØªÙƒÙ…", "Ù†Ø´ÙƒØ±ÙƒÙ… Ø¹Ù„Ù‰ ØªÙˆØ§ØµÙ„ÙƒÙ…"]
    casual_ack = ["ÙˆØµÙ„ØªÙ†ÙŠ", "Ø´ÙØª Ø±Ø³Ø§Ù„ØªÙƒ", "ØªÙ…Ø§Ù…"]
    
    formal_count = sum(1 for t in texts for a in formal_ack if a in t)
    casual_count = sum(1 for t in texts for a in casual_ack if a in t)
    
    if formal_count > casual_count:
        return "formal"
    elif casual_count > formal_count:
        return "casual"
    return "balanced"


def detect_personality_traits(texts: List[str]) -> List[str]:
    """Detect personality traits from writing style"""
    traits = []
    
    # Direct vs detailed
    avg_length = sum(len(t) for t in texts) / len(texts) if texts else 0
    if avg_length < 150:
        traits.append("direct")
    elif avg_length > 400:
        traits.append("detailed")
    
    # Empathetic markers
    empathy_markers = ["Ø£ÙÙ‡Ù…", "Ù…Ø¹Ùƒ Ø­Ù‚", "Ø£Ù‚Ø¯Ø±", "Ø¢Ø³Ù"]
    if sum(1 for t in texts for m in empathy_markers if m in t) > len(texts) * 0.2:
        traits.append("empathetic")
    
    # Friendly markers
    friendly_markers = ["ðŸ˜Š", "ðŸ‘", "Ù‡Ù„Ø§", "Ø­Ø¨ÙŠØ¨ÙŠ", "ÙŠØ§ Ø·ÙŠØ¨"]
    if sum(1 for t in texts for m in friendly_markers if m in t) > len(texts) * 0.2:
        traits.append("friendly")
    
    if not traits:
        traits.append("professional")
    
    return traits[:3]


# ============ Storage Functions ============

async def save_style_profile(profile: StyleProfile, db) -> bool:
    """Save style profile to database"""
    from db_helper import execute_sql
    
    try:
        await execute_sql(db, """
            INSERT OR REPLACE INTO style_profiles 
            (profile_id, license_id, profile_data, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            profile.profile_id,
            profile.license_id,
            json.dumps(profile.to_dict(), ensure_ascii=False),
            profile.created_at,
            profile.updated_at,
        ))
        return True
    except Exception as e:
        print(f"Error saving style profile: {e}")
        return False


async def get_style_profile(license_id: str, db) -> Optional[StyleProfile]:
    """Get style profile from database"""
    from db_helper import execute_sql
    
    try:
        result = await execute_sql(db, """
            SELECT profile_data FROM style_profiles
            WHERE license_id = ?
            ORDER BY updated_at DESC
            LIMIT 1
        """, (license_id,))
        
        if result and len(result) > 0:
            data = json.loads(result[0][0])
            return StyleProfile.from_dict(data)
    except Exception as e:
        print(f"Error getting style profile: {e}")
    
    return None


async def init_style_profiles_table(db):
    """Initialize the style_profiles table"""
    from db_helper import execute_sql
    
    await execute_sql(db, """
        CREATE TABLE IF NOT EXISTS style_profiles (
            profile_id TEXT PRIMARY KEY,
            license_id TEXT NOT NULL,
            profile_data TEXT NOT NULL,
            created_at TEXT,
            updated_at TEXT,
            UNIQUE(license_id)
        )
    """)
