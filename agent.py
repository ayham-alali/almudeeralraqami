"""
Al-Mudeer - LangGraph InboxCRM Agent
Implements: Ingest -> Classify -> Extract -> Draft pipeline
Optimized for low bandwidth with text-only responses
"""

import json
import re
from typing import TypedDict, Literal, Optional, Dict, Any, List
from models import update_daily_analytics
from dataclasses import dataclass
import httpx
import os
import asyncio

# Helper to fetch URL content
async def fetch_url_content(url: str) -> Optional[str]:
    """Fetch content from a URL (max 2000 chars)"""
    try:
        async with httpx.AsyncClient(timeout=10.0, follow_redirects=True) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            
            # Simple cleanup - remove HTML tags roughly
            text = resp.text
            # Remove scripts and styles
            text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)
            text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL)
            # Remove tags
            text = re.sub(r'<[^>]+>', ' ', text)
            # Compress whitespace
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text[:2000] # Limit context size
    except Exception as e:
        print(f"Failed to fetch URL {url}: {e}")
        return None

# LangGraph imports
from langgraph.graph import StateGraph, END

# Note: LLM configuration is centralized in services/llm_provider.py
# This file uses llm_generate() which handles OpenAI/Gemini failover

# Base system prompt for Arabic business context
BASE_SYSTEM_PROMPT = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙƒØªØ¨ÙŠ Ø°ÙƒÙŠ Ù„Ù„Ø´Ø±ÙƒØ§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ. ØªØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ù‡Ù†ÙŠ ÙˆÙ…Ù‡Ø°Ø¨.
ØªÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø¬ÙŠØ¯Ø§Ù‹ (Ø§Ù„Ø¹Ù…Ù„Ø©ØŒ Ø§Ù„Ø¹Ø§Ø¯Ø§ØªØŒ Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ØªØ®Ø§Ø·Ø¨).
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙˆØµÙŠØ§ØºØ© Ø±Ø¯ÙˆØ¯ Ù…Ù†Ø§Ø³Ø¨Ø©.
ÙƒÙ† Ù…ÙˆØ¬Ø²Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø±Ø¯ÙˆØ¯Ùƒ Ù„ØªÙˆÙÙŠØ± Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""


def build_system_prompt(preferences: Optional[Dict[str, Any]] = None) -> str:
    """
    Build a system prompt customized by workspace preferences.

    preferences comes from user_preferences table and may include:
    - tone: formal | friendly | custom
    - custom_tone_guidelines
    - business_name, industry, products_services
    - preferred_languages, reply_length, formality_level
    """
    if not preferences:
        return BASE_SYSTEM_PROMPT

    tone = (preferences.get("tone") or "formal").lower()
    custom_guidelines = (preferences.get("custom_tone_guidelines") or "").strip()

    # Tone description
    if tone == "friendly":
        tone_desc = "Ø§Ø³ØªØ®Ø¯Ù… Ù†Ø¨Ø±Ø© ÙˆØ¯ÙŠØ© ÙˆÙ‚Ø±ÙŠØ¨Ø© Ù„ÙƒÙ† Ù…Ø¹ Ø§Ø­ØªØ±Ø§Ù… Ù…Ù‡Ù†ÙŠØŒ ÙˆØªØ¬Ù†Ù‘Ø¨ Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©."
    elif tone == "custom" and custom_guidelines:
        tone_desc = custom_guidelines
    else:
        # formal or unknown
        tone_desc = "Ø§Ø³ØªØ®Ø¯Ù… Ù†Ø¨Ø±Ø© Ø±Ø³Ù…ÙŠØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø© Ø¨Ø¯ÙˆÙ† Ù…Ø¨Ø§Ù„ØºØ© ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù…Ù„Ø§Øª."

    business_name = preferences.get("business_name") or "Ø§Ù„Ø´Ø±ÙƒØ©"
    industry = preferences.get("industry") or ""
    products = preferences.get("products_services") or ""

    business_context_parts = [f"ØªØªØ­Ø¯Ø« Ø¨Ø§Ø³Ù… {business_name}."]
    if industry:
        business_context_parts.append(f"Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: {industry}.")
    if products:
        business_context_parts.append(f"Ø§Ù„Ø®Ø¯Ù…Ø§Øª / Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: {products}.")

    reply_length = (preferences.get("reply_length") or "").lower()
    if reply_length == "short":
        length_hint = "Ø§Ø­Ø±Øµ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù‚ØµÙŠØ±Ø§Ù‹ Ù‚Ø¯Ø± Ø§Ù„Ø¥Ù…ÙƒØ§Ù† (Ù…Ù† 2 Ø¥Ù„Ù‰ 3 Ø£Ø³Ø·Ø± ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹)."
    elif reply_length == "long":
        length_hint = "ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø¯ Ù…ÙØµÙ„Ø§Ù‹ Ø£ÙƒØ«Ø± Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©ØŒ Ù…Ø¹ Ø§Ù„Ù…Ø­Ø§ÙØ¸Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¶ÙˆØ­."
    else:
        length_hint = "Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø±Ø¯ Ù…ØªÙˆØ³Ø· ÙˆÙˆØ§Ø¶Ø­ (Ø­ÙˆØ§Ù„ÙŠ 3 Ø¥Ù„Ù‰ 6 Ø£Ø³Ø·Ø±)."

    return (
        BASE_SYSTEM_PROMPT
        + "\n\n"
        + "Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù…Ù„:\n"
        + " ".join(business_context_parts)
        + "\n\nØ£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n"
        + tone_desc
        + "\n"
        + length_hint
    )


class AgentState(TypedDict):
    """State for the InboxCRM agent"""
    # Input
    raw_message: str
    message_type: str  # email, whatsapp, general
    
    # Classification
    intent: str  # Ø§Ø³ØªÙØ³Ø§Ø±, Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©, Ø´ÙƒÙˆÙ‰, Ù…ØªØ§Ø¨Ø¹Ø©, Ø¹Ø±Ø¶, Ø£Ø®Ø±Ù‰
    urgency: str  # Ø¹Ø§Ø¬Ù„, Ø¹Ø§Ø¯ÙŠ, Ù…Ù†Ø®ÙØ¶
    sentiment: str  # Ø¥ÙŠØ¬Ø§Ø¨ÙŠ, Ù…Ø­Ø§ÙŠØ¯, Ø³Ù„Ø¨ÙŠ
    language: Optional[str]
    dialect: Optional[str]
    
    # Extraction
    sender_name: Optional[str]
    sender_contact: Optional[str]
    key_points: list[str]
    action_items: list[str]
    extracted_entities: dict  # dates, amounts, product names, etc.
    
    # Output
    summary: str
    draft_response: str
    suggested_actions: list[str]
    
    # Metadata
    error: Optional[str]
    processing_step: str

    # Preferences / context
    preferences: Optional[Dict[str, Any]]
    # Recent conversation history (plain text)
    conversation_history: Optional[str]
    
    # Multimodal support
    attachments: Optional[List[Dict[str, Any]]]


async def call_llm(
    prompt: str,
    system: Optional[str] = None,
    json_mode: bool = False,
    max_tokens: int = 600,
    attachments: Optional[List[Dict[str, Any]]] = None,
) -> Optional[str]:
    """
    Call LLM using multi-provider service with automatic failover.

    Provider chain: OpenAI -> Google Gemini -> Rule-based fallback
    
    Features:
    - Automatic failover between providers
    - Response caching to reduce API calls
    - Circuit breaker for failing providers
    - Exponential backoff for rate limiting
    
    Returns None if all providers fail (caller should use rule-based logic).
    """
    try:
        from services.llm_provider import llm_generate
        
        effective_system = system or BASE_SYSTEM_PROMPT
        
        response = await llm_generate(
            prompt=prompt,
            system=effective_system,
            json_mode=json_mode,
            max_tokens=max_tokens,
            temperature=0.3,
            attachments=attachments
        )
        
        return response
    except Exception as e:
        # If LLM fails, return None to trigger rule-based fallback
        print(f"LLM service error: {e}")
        return None



def rule_based_classify(message: str) -> dict:
    """Rule-based classification fallback (works offline)"""
    message_lower = message.lower()
    
    # Intent detection - order matters, more specific first
    intent = "Ø£Ø®Ø±Ù‰"
    
    # Help/assistance requests (common pattern)
    if any(word in message for word in ["Ù…Ø³Ø§Ø¹Ø¯", "Ø³Ø§Ø¹Ø¯", "ØªØ³Ø§Ø¹Ø¯", "help", "Ø£Ø­ØªØ§Ø¬"]):
        intent = "Ø·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø©"
    elif any(word in message for word in ["Ø³Ø¹Ø±", "ÙƒÙ…", "ØªÙƒÙ„ÙØ©", "Ø£Ø³Ø¹Ø§Ø±", "Ø«Ù…Ù†"]):
        intent = "Ø§Ø³ØªÙØ³Ø§Ø±"
    elif any(word in message for word in ["Ø£Ø±ÙŠØ¯", "Ø£Ø±ØºØ¨", "Ø·Ù„Ø¨", "Ø§Ø­ØªØ§Ø¬", "Ù†Ø±ÙŠØ¯", "Ø£Ø·Ù„Ø¨"]):
        intent = "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©"
    elif any(word in message for word in ["Ø´ÙƒÙˆÙ‰", "Ù…Ø´ÙƒÙ„Ø©", "Ù„Ù… ÙŠØ¹Ù…Ù„", "ØªØ£Ø®Ø±", "Ø³ÙŠØ¡", "Ø®Ø·Ø£"]):
        intent = "Ø´ÙƒÙˆÙ‰"
    elif any(word in message for word in ["Ù…ØªØ§Ø¨Ø¹Ø©", "Ø¨Ø®ØµÙˆØµ", "Ø§Ø³ØªÙƒÙ…Ø§Ù„", "ØªØ°ÙƒÙŠØ±"]):
        intent = "Ù…ØªØ§Ø¨Ø¹Ø©"
    elif any(word in message for word in ["Ø¹Ø±Ø¶", "Ø®ØµÙ…", "ØªØ®ÙÙŠØ¶", "ÙØ±ØµØ©"]):
        intent = "Ø¹Ø±Ø¶"
    # Marketing/Spam/Automated detection
    elif any(word in message for word in ["ÙƒÙˆØ¯", "Ø±Ù…Ø² ØªØ­Ù‚Ù‚", "otp", "code", "verification"]):
        intent = "Ø¢Ù„ÙŠ"
    elif any(word in message for word in ["Ø§Ø´ØªØ±Ùƒ", "Ø§Ø±Ø¨Ø­", "Ù…Ø¬Ø§Ù†Ø§", "Ø³Ø­Ø¨", "Ø¬ÙˆØ§Ø¦Ø²", "ØªØµÙÙŠØ©"]):
        intent = "ØªØ³ÙˆÙŠÙ‚"
    # Detect greetings/casual messages
    elif any(word in message for word in ["Ù…Ø±Ø­Ø¨", "Ø§Ù„Ø³Ù„Ø§Ù…", "Ø£Ù‡Ù„Ø§", "ØµØ¨Ø§Ø­", "Ù…Ø³Ø§Ø¡", "hi", "hello"]):
        intent = "ØªØ­ÙŠØ©"
    
    # Urgency detection
    urgency = "Ø¹Ø§Ø¯ÙŠ"
    if any(word in message for word in ["Ø¹Ø§Ø¬Ù„", "ÙÙˆØ±ÙŠ", "Ø§Ù„ÙŠÙˆÙ…", "Ø§Ù„Ø¢Ù†", "Ø¶Ø±ÙˆØ±ÙŠ"]):
        urgency = "Ø¹Ø§Ø¬Ù„"
    elif any(word in message for word in ["Ù„Ø§Ø­Ù‚Ø§Ù‹", "Ø¹Ù†Ø¯Ù…Ø§", "Ù…ØªÙ‰ Ù…Ø§"]):
        urgency = "Ù…Ù†Ø®ÙØ¶"
    
    # Sentiment detection
    sentiment = "Ù…Ø­Ø§ÙŠØ¯"
    if any(word in message for word in ["Ø´ÙƒØ±Ø§Ù‹", "Ù…Ù…ØªØ§Ø²", "Ø±Ø§Ø¦Ø¹", "Ø³Ø¹ÙŠØ¯", "Ù…Ø³Ø±ÙˆØ±"]):
        sentiment = "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ"
    elif any(word in message for word in ["ØºØ§Ø¶Ø¨", "Ù…Ø­Ø¨Ø·", "Ø³ÙŠØ¡", "Ù…Ø³ØªØ§Ø¡", "Ù„Ù„Ø£Ø³Ù"]):
        sentiment = "Ø³Ù„Ø¨ÙŠ"
    
    return {"intent": intent, "urgency": urgency, "sentiment": sentiment}


def extract_entities(message: str) -> dict:
    """Extract entities using regex patterns"""
    entities = {}
    
    # Phone numbers (Syrian/Arabic format)
    phone_patterns = [
        r'(?:00963|\+963|0)?9\d{8}',  # Syrian mobile
        r'(?:00963|\+963|0)?11\d{7}',  # Damascus landline
        r'\d{3}[-.\s]?\d{3}[-.\s]?\d{4}',  # General format
    ]
    phones = []
    for pattern in phone_patterns:
        phones.extend(re.findall(pattern, message))
    if phones:
        entities["phones"] = list(set(phones))
    
    # Email
    emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', message)
    if emails:
        entities["emails"] = emails
    
    # Dates (Arabic format)
    dates = re.findall(r'\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4}', message)
    if dates:
        entities["dates"] = dates
    
    # Money amounts
    amounts = re.findall(r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ù„\.Ø³|Ù„ÙŠØ±Ø©|Ø¯ÙˆÙ„Ø§Ø±|\$|USD)', message)
    if amounts:
        entities["amounts"] = amounts
    
    # Extract possible name (after Ø§Ù„Ø³ÙŠØ¯/Ø§Ù„Ø³ÙŠØ¯Ø©/Ø§Ù„Ø£Ø³ØªØ§Ø°)
    name_match = re.search(r'(?:Ø§Ù„Ø³ÙŠØ¯|Ø§Ù„Ø³ÙŠØ¯Ø©|Ø§Ù„Ø£Ø³ØªØ§Ø°|Ø§Ù„Ø£Ø³ØªØ§Ø°Ø©|Ø£Ø®ÙŠ|Ø£Ø®ØªÙŠ)\s+([\u0600-\u06FF\s]+)', message)
    if name_match:
        entities["mentioned_name"] = name_match.group(1).strip()
    
    return entities


def generate_rule_based_response(state: dict) -> str:
    """Generate a human-like draft response based on intent"""
    intent = state.get("intent", "Ø£Ø®Ø±Ù‰")
    sender = state.get("sender_name") or ""  # Don't use formal address for unknown
    raw_message = state.get("raw_message", "")
    
    # For short/simple messages, use conversational style
    if len(raw_message.strip()) < 50:
        # Short message - be conversational
        templates = {
            "Ø·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø©": "Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸ‘‹\nØ¨Ø§Ù„Ø·Ø¨Ø¹ØŒ Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ.\nÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø®Ø¯Ù…ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
            
            "ØªØ­ÙŠØ©": "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! ğŸ˜Š\nØ³Ø¹ÙŠØ¯ÙˆÙ† Ø¨ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹Ù†Ø§.\nÙƒÙŠÙ Ù†Ù‚Ø¯Ø± Ù†Ø³Ø§Ø¹Ø¯ÙƒØŸ",
            
            "Ø§Ø³ØªÙØ³Ø§Ø±": "Ù…Ø±Ø­Ø¨Ø§Ù‹!\nØ´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ.\nÙ…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ø£ÙƒØ«Ø± Ø¹Ù† Ø§Ù„Ù„ÙŠ ØªØ­ØªØ§Ø¬Ù‡ØŸ",
            
            "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©": "Ø£Ù‡Ù„Ø§Ù‹!\nØªÙ…Ø§Ù…ØŒ ÙˆØµÙ„Ù†Ø§ Ø·Ù„Ø¨Ùƒ.\nÙ…Ù…ÙƒÙ† ØªØ¹Ø·ÙŠÙ†Ø§ ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø±ØŸ",
            
            "Ø´ÙƒÙˆÙ‰": "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ\nØ¢Ø³ÙÙŠÙ† ØªØ³Ù…Ø¹ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù…! ğŸ˜”\nÙ…Ù…ÙƒÙ† ØªÙˆØ¶Ø­ Ù„Ù†Ø§ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ø¨Ø§Ù„ØªÙØµÙŠÙ„ Ø¹Ø´Ø§Ù† Ù†Ø­Ù„Ù‡Ø§ Ø¨Ø£Ø³Ø±Ø¹ ÙˆÙ‚ØªØŸ",
            
            "Ø£Ø®Ø±Ù‰": "Ù…Ø±Ø­Ø¨Ø§Ù‹! ğŸ‘‹\nØ´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹Ù†Ø§.\nÙƒÙŠÙ Ù†Ù‚Ø¯Ø± Ù†Ø®Ø¯Ù…Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
        }
    else:
        # Longer/formal messages - professional but still warm
        sender_greeting = f"Ø£Ù‡Ù„Ø§Ù‹{' ' + sender if sender and sender != 'None' else ''}ØŒ" if sender and sender not in ['None', ''] else "Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ"
        
        templates = {
            "Ø·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø©": f"""{sender_greeting}

Ø¨Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ù†Ù‚Ø¯Ø± Ù†Ø³Ø§Ø¹Ø¯Ùƒ! âœ¨
ÙˆØ¶Ù‘Ø­Ù†Ø§ Ø¨Ø±Ø³Ø§Ù„ØªÙƒ ÙˆØ³Ù†Ù‚ÙˆÙ… Ø¨Ø§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙƒ Ø¨Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù…Ù…ÙƒÙ†Ø©.

Ù†Ø­Ù† Ø¨Ø®Ø¯Ù…ØªÙƒ Ø¯Ø§Ø¦Ù…Ø§Ù‹.""",
            
            "ØªØ­ÙŠØ©": f"""{sender_greeting}

Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ğŸ˜Š
Ø³Ø¹Ø¯Ø§Ø¡ Ø¬Ø¯Ø§Ù‹ Ø¨ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹Ù†Ø§.

ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ""",

            "Ø§Ø³ØªÙØ³Ø§Ø±": f"""{sender_greeting}

Ø´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„Ùƒ ÙˆØ§Ø³ØªÙØ³Ø§Ø±Ùƒ.

Ø¨Ø®ØµÙˆØµ Ù…Ø§ Ø°ÙƒØ±ØªÙ‡ØŒ Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ Ø£Ù‚Ø±Ø¨ ÙˆÙ‚Øª.
Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙƒ ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©ØŒ Ø´Ø§Ø±ÙƒÙ†Ø§ Ø¨Ù‡Ø§.

Ù…Ø¹ ØªØ­ÙŠØ§ØªÙ†Ø§ ğŸŒŸ""",
            
            "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©": f"""{sender_greeting}

Ø´ÙƒØ±Ø§Ù‹ Ù„Ø«Ù‚ØªÙƒ Ø¨Ù†Ø§! ğŸ’«

ÙˆØµÙ„Ù†Ø§ Ø·Ù„Ø¨Ùƒ ÙˆØ³Ù†ØªÙˆØ§ØµÙ„ Ù…Ø¹Ùƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹ Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„ØªÙØ§ØµÙŠÙ„.

Ù†Ø³Ø¹Ø¯ Ø¨Ø®Ø¯Ù…ØªÙƒ.""",
            
            "Ø´ÙƒÙˆÙ‰": f"""{sender_greeting}

Ù†Ø¹ØªØ°Ø± Ø¬Ø¯Ø§Ù‹ Ø¹Ù† Ø£ÙŠ Ø¥Ø²Ø¹Ø§Ø¬ ÙˆØ§Ø¬Ù‡ØªÙ‡. ğŸ˜”

Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙƒ Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹ Ù„Ù†Ø§ ÙˆØ³Ù†Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰.
Ø³ÙŠØªÙˆØ§ØµÙ„ Ù…Ø¹Ùƒ Ø£Ø­Ø¯ ÙØ±ÙŠÙ‚Ù†Ø§ Ù‚Ø±ÙŠØ¨Ø§Ù‹.

Ø´ÙƒØ±Ø§Ù‹ Ù„ØµØ¨Ø±Ùƒ.""",
            
            "Ù…ØªØ§Ø¨Ø¹Ø©": f"""{sender_greeting}

Ø´ÙƒØ±Ø§Ù‹ Ù„Ù…ØªØ§Ø¨Ø¹ØªÙƒ Ù…Ø¹Ù†Ø§.

Ø³Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙˆØ¥Ø·Ù„Ø§Ø¹Ùƒ Ø¹Ù„Ù‰ Ø¢Ø®Ø± Ø§Ù„Ù…Ø³ØªØ¬Ø¯Ø§Øª ÙÙŠ Ø£Ù‚Ø±Ø¨ ÙØ±ØµØ©.

Ù†Ù‚Ø¯Ø± ØªÙˆØ§ØµÙ„Ùƒ Ø§Ù„Ù…Ø³ØªÙ…Ø±. ğŸ™""",
            
            "Ø¹Ø±Ø¶": f"""{sender_greeting}

Ø´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„Ùƒ ÙˆØ¹Ø±Ø¶Ùƒ Ø§Ù„ÙƒØ±ÙŠÙ….

Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø¯Ø±Ø§Ø³ØªÙ‡ ÙˆØ§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙƒ Ù‚Ø±ÙŠØ¨Ø§Ù‹.

Ù…Ø¹ Ø§Ù„ØªÙ‚Ø¯ÙŠØ±.""",
            
            "Ø£Ø®Ø±Ù‰": f"""{sender_greeting}

Ø´ÙƒØ±Ø§Ù‹ Ù„ØªÙˆØ§ØµÙ„Ùƒ Ù…Ø¹Ù†Ø§! ğŸŒŸ

ÙˆØµÙ„ØªÙ†Ø§ Ø±Ø³Ø§Ù„ØªÙƒ ÙˆØ³Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§ ÙˆØ§Ù„Ø±Ø¯ Ø¹Ù„ÙŠÙƒ ÙÙŠ Ø£Ù‚Ø±Ø¨ ÙˆÙ‚Øª.

Ù†Ø­Ù† Ø³Ø¹Ø¯Ø§Ø¡ Ø¨Ø®Ø¯Ù…ØªÙƒ."""
        }
    
    return templates.get(intent, templates["Ø£Ø®Ø±Ù‰"])


# ============ LangGraph Nodes ============

async def ingest_node(state: AgentState) -> AgentState:
    """Step 1: Ingest and clean the message"""
    state["processing_step"] = "Ø§Ø³ØªÙ„Ø§Ù…"
    
    # Update analytics for received message
    if state.get("preferences") and state["preferences"].get("license_key_id"):
        try:
            from models import update_daily_analytics
            # Note: We use asyncio.create_task to not block the agent flow
            import asyncio
            asyncio.create_task(update_daily_analytics(
                license_id=state["preferences"]["license_key_id"],
                messages_received=1
            ))
        except Exception as e:
            print(f"Analytics update failed: {e}")
    
    # Clean the message
    raw = state["raw_message"].strip()
    
    # Detect message type if not specified
    if not state.get("message_type"):
        if "@" in raw and "subject" in raw.lower():
            state["message_type"] = "email"
        elif any(x in raw for x in ["ÙˆØ§ØªØ³Ø§Ø¨", "whatsapp", "ğŸ“±"]):
            state["message_type"] = "whatsapp"
        else:
            state["message_type"] = "general"
            
    # Link Browsing: Detect and fetch URLs
    # Pattern for http/https URLs
    url_pattern = r'https?://[^\s<>"]+|www\.[^\s<>"]+'
    urls = re.findall(url_pattern, raw)
    
    if urls:
        # Fetch first URL only to save tokens/time
        url = urls[0]
        print(f"Detected URL: {url} - fetching content...")
        
        # We need to run this async, but ingest_node is async so it fits
        content = await fetch_url_content(url)
        
        if content:
            # Append to raw message as context for subsequent nodes
            state["raw_message"] += f"\n\n[System: Content fetching from {url}]\n{content}"
            print(f"Added {len(content)} chars of context from URL")
    
    return state


async def classify_node(state: AgentState) -> AgentState:
    """Step 2: Classify intent, urgency, and sentiment"""
    state["processing_step"] = "ØªØµÙ†ÙŠÙ"
    
    # Try LLM first â€“ structured JSON output in Arabic business context
    history_block = ""
    if state.get("conversation_history"):
        history_block = f"\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø¯Ù…):\n{state['conversation_history']}\n"

    prompt = f"""Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆÙ„ØºØ§Øª Ø£Ø®Ø±Ù‰.
Ø­Ù„Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ£Ø¹Ø·Ù†ÙŠ:
3. Ø­Ù„Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ£Ø¹Ø·Ù†ÙŠ:
4. 1. Ø§Ù„Ù†ÙŠØ© (intent): Ø§Ø³ØªÙØ³Ø§Ø±ØŒ Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©ØŒ Ø´ÙƒÙˆÙ‰ØŒ Ù…ØªØ§Ø¨Ø¹Ø©ØŒ Ø¹Ø±Ø¶ØŒ ØªØ³ÙˆÙŠÙ‚ (Ù„Ù„Ù…ØªØ·ÙÙ„ÙŠÙ†)ØŒ Ø¢Ù„ÙŠ (OTP/ØªÙ†Ø¨ÙŠÙ‡Ø§Øª)ØŒ Ø£Ùˆ Ø£Ø®Ø±Ù‰
2. Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (urgency): Ø¹Ø§Ø¬Ù„ØŒ Ø¹Ø§Ø¯ÙŠØŒ Ù…Ù†Ø®ÙØ¶
3. Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (sentiment): Ø¥ÙŠØ¬Ø§Ø¨ÙŠØŒ Ù…Ø­Ø§ÙŠØ¯ØŒ Ø³Ù„Ø¨ÙŠ
4. Ø§Ù„Ù„ØºØ© (language): ar, en, fr, Ø£Ùˆ Ø±Ù…Ø² ISO Ø¥Ù† Ø£Ù…ÙƒÙ†
5. Ø§Ù„Ù„Ù‡Ø¬Ø© (dialect): Ø³ÙˆØ±ÙŠØŒ Ø³Ø¹ÙˆØ¯ÙŠØŒ Ù…ØµØ±ÙŠØŒ Ø®Ù„ÙŠØ¬ÙŠØŒ ÙØµØ­Ù‰ØŒ Ø£Ùˆ Other

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŒ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„Ø­ÙƒÙ… Ù…Ù† ÙƒÙ„Ù…Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø·.
{history_block}
Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø§Ù„ÙŠ:
{state['raw_message']}

Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{"intent": "Ø§Ø³ØªÙØ³Ø§Ø±", "urgency": "Ø¹Ø§Ø¯ÙŠ", "sentiment": "Ù…Ø­Ø§ÙŠØ¯", "language": "ar", "dialect": "Ø´Ø§Ù…ÙŠ"}}"""

    llm_response = await call_llm(
        prompt,
        system=build_system_prompt(state.get("preferences")),
        json_mode=True,
        attachments=state.get("attachments")
    )
    
    if llm_response:
        try:
            classification = json.loads(llm_response)
            state["intent"] = classification.get("intent", "Ø£Ø®Ø±Ù‰")
            state["urgency"] = classification.get("urgency", "Ø¹Ø§Ø¯ÙŠ")
            state["sentiment"] = classification.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯")
            state["language"] = classification.get("language") or "ar"
            state["dialect"] = classification.get("dialect")
            return state
        except json.JSONDecodeError:
            pass
    
    # Fallback: Mark as pending retry (no rule-based fallback)
    # This ensures only Gemini-quality responses are used
    state["intent"] = "pending"
    state["urgency"] = "Ø¹Ø§Ø¯ÙŠ"
    state["sentiment"] = "Ù…Ø­Ø§ÙŠØ¯"
    state["error"] = "LLM unavailable - will retry"
    
    return state


async def extract_node(state: AgentState) -> AgentState:
    """Step 3: Extract key information"""
    state["processing_step"] = "Ø§Ø³ØªØ®Ø±Ø§Ø¬"
    
    # Extract entities using regex (reliable, no LLM needed)
    entities = extract_entities(state["raw_message"])
    state["extracted_entities"] = entities
    
    # Set sender info from entities if found
    if entities.get("mentioned_name"):
        state["sender_name"] = entities["mentioned_name"]
    if entities.get("emails"):
        state["sender_contact"] = entities["emails"][0]
    elif entities.get("phones"):
        state["sender_contact"] = entities["phones"][0]
    
    # Try LLM for key points extraction
    history_block = ""
    if state.get("conversation_history"):
        history_block = f"\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù…Ø¹ Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù‚Ø¯Ù…):\n{state['conversation_history']}\n"

    prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙŠØ¯Ø¹Ù… ÙØ±ÙŠÙ‚ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡.
Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ø³ØªØ®Ø±Ø¬ Ø¨Ø§Ø®ØªØµØ§Ø±:
1. Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ°ÙƒØ±Ù‡Ø§ Ø§Ù„Ø¹Ù…ÙŠÙ„ (3 Ù†Ù‚Ø§Ø· ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰).
2. Ø£Ù‡Ù… Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø£Ùˆ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠÙ†Ø¨ØºÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙØ±ÙŠÙ‚ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ù‡Ø§.

ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ Ø¨Ø³ÙŠØ·Ø© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©.
{history_block}
Ù†Øµ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
{state['raw_message']}

Ø£Ø±Ø¬Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{"key_points": ["Ù†Ù‚Ø·Ø© Ù…Ø®ØªØµØ±Ø© 1", "Ù†Ù‚Ø·Ø© Ù…Ø®ØªØµØ±Ø© 2"], "action_items": ["Ø¥Ø¬Ø±Ø§Ø¡ ÙˆØ§Ø¶Ø­ 1"]}}"""

    llm_response = await call_llm(
        prompt,
        system=build_system_prompt(state.get("preferences")),
        json_mode=True,
    )
    
    if llm_response:
        try:
            extracted = json.loads(llm_response)
            state["key_points"] = extracted.get("key_points", [])
            state["action_items"] = extracted.get("action_items", [])
            return state
        except json.JSONDecodeError:
            pass
    
    # Fallback: Skip extraction if LLM failed (will retry later)
    if not state.get("key_points"):
        state["key_points"] = []
    if not state.get("action_items"):
        state["action_items"] = []
    
    return state


async def draft_node(state: AgentState) -> AgentState:
    """Step 4: Draft a human-like response"""
    state["processing_step"] = "ØµÙŠØ§ØºØ©"
    
    # Fix None sender name
    sender = state.get("sender_name")
    if not sender or sender == "None":
        sender = ""  # Don't use formal address for unknown
    
    intent = state.get("intent", "Ø£Ø®Ø±Ù‰")
    key_points = state.get("key_points", [])
    raw_message = state.get("raw_message", "")
    dialect = state.get("dialect", "ÙØµØ­Ù‰")
    
    # Determine response style based on message length
    is_casual = len(raw_message.strip()) < 50
    
    # Build conversation history context
    history_block = ""
    if state.get("conversation_history"):
        history_block = f"\nPrevious conversation context:\n{state['conversation_history']}\n"
    
    # Get detected language
    language = state.get("language", "ar")
    
    # Build language and dialect instructions
    if language and language != "ar":
        # Non-Arabic language - respond in same language
        language_names = {
            "en": "English",
            "fr": "French", 
            "es": "Spanish",
            "de": "German",
            "tr": "Turkish",
        }
        lang_name = language_names.get(language, language.upper())
        
        prompt = f"""You are a friendly, professional customer service representative. You speak naturally like a real person, not a robot.

ğŸ¯ Your task: Write a natural, direct response to the customer's message.

ğŸ—£ï¸ IMPORTANT: Respond in {lang_name} (the same language as the customer)!

âœ… Do:
- Be friendly, direct, and natural
- Answer what the customer asked/requested directly
- Use simple, clear language
- You can use one or two emojis if appropriate ğŸ˜Š
{"- Be very concise (2-3 lines only)" if is_casual else "- Keep the response appropriate to the message length (4-6 lines)"}

âŒ Don't:
- Don't use overly formal phrases like "Dear Sir/Madam"
- Don't say "Your message has been received" (boring and robotic)
- Don't end with "Customer Service Team" (too formal)
- Don't repeat the same routine phrases
- Don't say "I am an AI" or "I cannot" - just respond naturally

ğŸ“ Customer's message:
\"{raw_message}\"

ğŸ“Š Message analysis:
- Type: {intent}
- Language: {lang_name}
- Key points: {', '.join(key_points) if key_points else 'General message'}
{f"- Customer name: {sender}" if sender else ""}
{history_block}

âœï¸ Write your response directly in {lang_name} (no explanation):"""

    else:
        # Arabic - handle dialects
        dialect_instruction = ""
        if dialect and dialect != "ÙØµØ­Ù‰":
            dialect_examples = {
                "Ø³Ø¹ÙˆØ¯ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©/Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'ÙˆØ´ ØªØ­ØªØ§Ø¬ØŸ'ØŒ 'ØªÙ…Ø§Ù…'ØŒ 'Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡'ØŒ 'ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©'ØŒ 'ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ'",
                "Ø®Ù„ÙŠØ¬ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø´Ù„ÙˆÙ†ÙƒØŸ'ØŒ 'Ø²ÙŠÙ†'ØŒ 'ÙˆØ§Ø¬Ø¯'ØŒ 'ÙŠØ§ Ù‡Ù„Ø§'ØŒ 'ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø®Ø¯Ù…ÙƒØŸ'",
                "Ù…ØµØ±ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø¥Ø²ÙŠÙƒØŸ'ØŒ 'ØªÙ…Ø§Ù…'ØŒ 'Ø¹Ø§ÙŠØ² Ø¥ÙŠÙ‡ØŸ'ØŒ 'Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¥Ø²Ø§ÙŠØŸ'ØŒ 'Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©'",
                "Ø´Ø§Ù…ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø´Ø§Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'ÙƒÙŠÙÙƒØŸ'ØŒ 'Ø´Ùˆ Ø¨Ø¯ÙƒØŸ'ØŒ 'Ù…Ù†ÙŠØ­'ØŒ 'Ù‡Ù„Ù‚'ØŒ 'ÙƒØªÙŠØ± Ù…Ù†ÙŠØ­'",
                "Ø³ÙˆØ±ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³ÙˆØ±ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø´Ùˆ Ø¨Ø¯ÙƒØŸ'ØŒ 'ÙƒÙŠÙÙƒØŸ'ØŒ 'Ù…Ù†ÙŠØ­'ØŒ 'Ù‡Ù„Ù‚'ØŒ 'Ù„ÙŠÙƒ'",
            }
            dialect_instruction = dialect_examples.get(dialect, f"Ø§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬Ø© {dialect} ÙÙŠ Ø§Ù„Ø±Ø¯ Ø¥Ù† Ø£Ù…ÙƒÙ†.")

        prompt = f"""Ø£Ù†Øª Ù…Ù…Ø«Ù„ Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ ÙˆØ¯ÙˆØ¯ ÙˆØ·Ø¨ÙŠØ¹ÙŠ. ØªØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙƒØ¥Ù†Ø³Ø§Ù† Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù„Ø³Øª Ø±ÙˆØ¨ÙˆØªØ§Ù‹.

ğŸ¯ Ù…Ù‡Ù…ØªÙƒ: Ø§ÙƒØªØ¨ Ø±Ø¯Ø§Ù‹ Ø·Ø¨ÙŠØ¹ÙŠØ§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„.

ğŸ—£ï¸ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {dialect}
{dialect_instruction if dialect_instruction else "Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ Ù…Ø¨Ø³Ù‘Ø·Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…."}

âœ… Ø§ÙØ¹Ù„:
- ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙˆØ·Ø¨ÙŠØ¹ÙŠØ§Ù‹
- Ø±Ø¯ Ø¹Ù„Ù‰ Ù…Ø§ Ø³Ø£Ù„Ù‡/Ø·Ù„Ø¨Ù‡ Ø§Ù„Ø¹Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø©
- Ø·Ø§Ø¨Ù‚ Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø±Ø¯Ùƒ (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹!)
- ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥ÙŠÙ…ÙˆØ¬ÙŠ ÙˆØ§Ø­Ø¯ Ø£Ùˆ Ø§Ø«Ù†ÙŠÙ† Ø¥Ù† Ù…Ù†Ø§Ø³Ø¨ ğŸ˜Š
- Ø¥Ø°Ø§ Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø´Ø®ØµÙŠØ§Ù‹ØŒ Ø±Ø¯ Ø¨Ù„Ø·Ù ÙˆØ­ÙˆÙ‘Ù„ Ø§Ù„Ø­Ø¯ÙŠØ« Ù„Ù„Ø®Ø¯Ù…Ø§Øª
{"- ÙƒÙ† Ù…ÙˆØ¬Ø²Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ (Ø³Ø·Ø±ÙŠÙ† Ø£Ùˆ Ø«Ù„Ø§Ø«Ø© ÙÙ‚Ø·)" if is_casual else "- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ Ù…Ù„Ø§Ø¦Ù…Ø§Ù‹ Ù„Ø·ÙˆÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© (4-6 Ø£Ø³Ø·Ø±)"}

âŒ Ù„Ø§ ØªÙØ¹Ù„:
- Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… "Ø§Ù„Ø³ÙŠØ¯/Ø§Ù„Ø³ÙŠØ¯Ø© Ø§Ù„Ù…Ø­ØªØ±Ù…/Ø©" (Ø±Ø³Ù…ÙŠ Ø¬Ø¯Ø§Ù‹!)
- Ù„Ø§ ØªÙ‚Ù„ "ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø±Ø³Ø§Ù„ØªÙƒ" (Ù…Ù…Ù„ ÙˆØ±ÙˆØ¨ÙˆØªÙŠ)
- Ù„Ø§ ØªÙ†Ù‡ Ø§Ù„Ø±Ø¯ Ø¨Ù€ "ÙØ±ÙŠÙ‚ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡" (Ø±Ø³Ù…ÙŠ Ø¬Ø¯Ø§Ù‹)
- Ù„Ø§ ØªÙƒØ±Ø± Ù†ÙØ³ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø±ÙˆØªÙŠÙ†ÙŠØ©
- Ù„Ø§ ØªÙ‚Ù„ "Ø£Ù†Ø§ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ" Ø£Ùˆ "Ù„ÙŠØ³ Ù„Ø¯ÙŠ Ø§Ù„Ù‚Ø¯Ø±Ø©" - ÙÙ‚Ø· Ø±Ø¯ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ
- Ù„Ø§ ØªØ±Ø¯ Ø¨Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬Ø© Ù…Ø­Ù„ÙŠØ©

ğŸ“ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„:
\"{raw_message}\"

ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©:
- Ù†ÙˆØ¹: {intent}
- Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {dialect}
- Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©: {', '.join(key_points) if key_points else 'Ø±Ø³Ø§Ù„Ø© Ø¹Ø§Ù…Ø©'}
{f"- Ø§Ø³Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„: {sender}" if sender else ""}
{history_block}

âœï¸ Ø§ÙƒØªØ¨ Ø§Ù„Ø±Ø¯ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ù†ÙØ³ Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø´Ø±Ø­):"""

    llm_response = await call_llm(
        prompt,
        system=build_system_prompt(state.get("preferences")),
        json_mode=False,
        max_tokens=1200,  # Arabic needs more tokens - increased from 800
        attachments=state.get("attachments")
    )
    
    # Lower threshold to 15 - accept short but valid responses
    if llm_response and len(llm_response.strip()) > 15:
        state["draft_response"] = llm_response.strip()
    else:
        # No fallback to generic templates - use placeholder for retry
        # This ensures only Gemini-quality responses are shown to users
        state["draft_response"] = "â³ Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹..."
        state["error"] = "LLM unavailable - pending retry"
        
    # Update analytics for reply generation
    if state.get("preferences") and state["preferences"].get("license_key_id"):
        try:
            from models import update_daily_analytics
            # Note: We use asyncio.create_task to not block the agent flow
            import asyncio
            asyncio.create_task(update_daily_analytics(
                license_id=state["preferences"]["license_key_id"],
                messages_replied=1,
                sentiment=state.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯")
            ))
        except Exception as e:
            print(f"Analytics reply update failed: {e}")
    
    # Generate a cleaner summary (avoid showing "None")
    sender_display = sender if sender else "Ø¹Ù…ÙŠÙ„"
    state["summary"] = f"Ø±Ø³Ø§Ù„Ø© {intent} Ù…Ù† {sender_display}. Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {state.get('sentiment', 'Ù…Ø­Ø§ÙŠØ¯')}. Ø§Ù„Ø£Ù‡Ù…ÙŠØ©: {state.get('urgency', 'Ø¹Ø§Ø¯ÙŠ')}."
    
    # Suggested actions based on intent
    actions_map = {
        "Ø·Ù„Ø¨ Ù…Ø³Ø§Ø¹Ø¯Ø©": ["Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…ÙŠÙ„", "ØªÙˆØ¶ÙŠØ­ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"],
        "ØªØ­ÙŠØ©": ["Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¨Ø§Ù„Ø¹Ù…ÙŠÙ„", "Ø¨Ø¯Ø¡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"],
        "Ø§Ø³ØªÙØ³Ø§Ø±": ["Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±", "Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"],
        "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©": ["Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨ Ø¬Ø¯ÙŠØ¯", "ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¹Ø¯", "Ø¥Ø±Ø³Ø§Ù„ Ø¹Ø±Ø¶ Ø³Ø¹Ø±"],
        "Ø´ÙƒÙˆÙ‰": ["ØªØµØ¹ÙŠØ¯ Ù„Ù„Ù…Ø¯ÙŠØ±", "ÙØªØ­ ØªØ°ÙƒØ±Ø© Ø¯Ø¹Ù…", "Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¹Ù…ÙŠÙ„"],
        "Ù…ØªØ§Ø¨Ø¹Ø©": ["ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ù„Ø¨", "Ø¥Ø±Ø³Ø§Ù„ ØªÙ‚Ø±ÙŠØ±"],
        "Ø¹Ø±Ø¶": ["Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ø¹Ø±Ø¶", "ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ù…Ø´ØªØ±ÙŠØ§Øª"],
        "Ø£Ø®Ø±Ù‰": ["Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©", "ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ø³Ø§Ù„Ø©"]
    }
    state["suggested_actions"] = actions_map.get(intent, actions_map["Ø£Ø®Ø±Ù‰"])
    
    return state


# ============ Build the Graph ============

def create_inbox_agent():
    """Create the InboxCRM LangGraph agent"""
    
    # Create the graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("ingest", ingest_node)
    workflow.add_node("classify", classify_node)
    workflow.add_node("extract", extract_node)
    workflow.add_node("draft", draft_node)
    
    # Define edges (linear pipeline)
    # Define conditional routing
    def route_after_classify(state: AgentState):
        """Route to extract or END based on intent"""
        intent = state.get("intent", "Ø£Ø®Ø±Ù‰")
        if intent in ["ØªØ³ÙˆÙŠÙ‚", "Ø¢Ù„ÙŠ", "spam", "marketing", "automated"]:
            return "end"
        return "extract"

    # Define edges
    workflow.set_entry_point("ingest")
    workflow.add_edge("ingest", "classify")
    workflow.add_conditional_edges(
        "classify",
        route_after_classify,
        {
            "extract": "extract",
            "end": END
        }
    )
    workflow.add_edge("extract", "draft")
    workflow.add_edge("draft", END)
    
    # Compile
    return workflow.compile()


# Singleton agent instance
_agent = None

def get_agent():
    """Get or create the agent instance"""
    global _agent
    if _agent is None:
        _agent = create_inbox_agent()
    return _agent


async def process_message(
    message: str,
    message_type: str = None,
    sender_name: str = None,
    sender_contact: str = None,
    sender_city: str = None,
    preferences: Optional[Dict[str, Any]] = None,
    conversation_history: Optional[str] = None,
    attachments: Optional[List[Dict[str, Any]]] = None,
) -> dict:
    """Process a message through the InboxCRM pipeline"""
    
    agent = get_agent()
    
    # Initial state
    initial_state: AgentState = {
        "raw_message": message,
        "message_type": message_type or "general",
        "intent": "",
        "urgency": "",
        "sentiment": "",
        "sender_name": sender_name,
        "sender_contact": sender_contact,
        "key_points": [],
        "action_items": [],
        "extracted_entities": {},
        "summary": "",
        "draft_response": "",
        "suggested_actions": [],
        "error": None,
        "processing_step": "",
        "preferences": preferences,
        "preferences": preferences,
        "conversation_history": conversation_history,
        "attachments": attachments,
    }
    
    try:
        # Run the agent
        final_state = await agent.ainvoke(initial_state)
        return {
            "success": True,
            "data": {
                "intent": final_state["intent"],
                "urgency": final_state["urgency"],
                "sentiment": final_state["sentiment"],
                "language": final_state.get("language"),
                "dialect": final_state.get("dialect"),
                "sender_name": final_state["sender_name"],
                "sender_contact": final_state["sender_contact"],
                "key_points": final_state["key_points"],
                "action_items": final_state["action_items"],
                "extracted_entities": final_state["extracted_entities"],
                "summary": final_state["summary"],
                "draft_response": final_state["draft_response"],
                "suggested_actions": final_state["suggested_actions"],
                "message_type": final_state["message_type"]
            }
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"
        }

