"""
Al-Mudeer Enhanced AI Agent
Human-like responses with persona support, anti-robotic patterns, and style learning
"""

import json
import json_repair
import re
from typing import TypedDict, Optional, Dict, Any
import os

from langgraph.graph import StateGraph, END

from personas import (
    get_persona,
    get_persona_for_intent,
    build_persona_prompt,
    get_persona_temperature,
    get_random_greeting,
    get_random_closing,
)
from humanize import (
    build_few_shot_prompt,
    remove_robotic_phrases,
    get_dynamic_temperature,
    check_response_quality,
    ROBOTIC_PHRASES,
)
from models import update_daily_analytics
import asyncio
from services.knowledge_base import get_knowledge_base
from message_filters import apply_filters



# Note: LLM configuration is centralized in services/llm_provider.py
# This file uses llm_generate() which handles OpenAI/Gemini failover


class EnhancedAgentState(TypedDict):
    """Enhanced state with persona, style learning, and quality tracking"""
    # Input
    raw_message: str
    message_type: str
    
    # Classification
    intent: str
    urgency: str
    sentiment: str
    language: Optional[str]
    dialect: Optional[str]
    
    # Extraction
    sender_name: Optional[str]
    sender_contact: Optional[str]
    key_points: list
    action_items: list
    extracted_entities: dict
    
    # Customer context
    customer_history: Optional[Dict[str, Any]]
    relationship_level: str  # new, returning, vip
    
    # Persona
    persona_name: str
    persona_auto_selected: bool
    
    # Style Learning (new)
    use_learned_style: bool  # Whether to use learned style
    style_profile: Optional[Dict[str, Any]]  # Learned style profile
    
    # Output
    summary: str
    draft_response: str
    suggested_actions: list
    
    # Quality
    response_quality_score: int
    response_quality_issues: list
    
    # Metadata
    error: Optional[str]
    processing_step: str
    preferences: Optional[Dict[str, Any]]
    conversation_history: Optional[str]
    
    # Market-Ready Features (New)
    knowledge_facts: list  # RAG facts
    tool_calls: list  # External tool executions
    needs_human_intervention: bool  # High-priority escalation flag


async def call_llm_enhanced(
    prompt: str,
    system: str,
    temperature: float = 0.3,
    json_mode: bool = False,
    max_tokens: int = 600,
    tools: Optional[list] = None,
) -> Any:
    """Enhanced LLM call using centralized llm_generate service."""
    try:
        from services.llm_provider import llm_generate
        
        response = await llm_generate(
            prompt=prompt,
            system=system,
            json_mode=json_mode,
            max_tokens=max_tokens,
            temperature=temperature,
            tools=tools
        )
        
        return response
    except Exception as e:
        print(f"LLM call failed: {e}")
        return None


# ============ Enhanced Pipeline Nodes ============

async def enhanced_classify_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Classify with advanced analysis and dialect awareness"""
    state["processing_step"] = "ØªØµÙ†ÙŠÙ"
    
    # Update analytics (received)
    if state.get("preferences") and state["preferences"].get("license_key_id"):
        try:
            asyncio.create_task(update_daily_analytics(
                license_id=state["preferences"]["license_key_id"],
                messages_received=1
            ))
        except Exception as e:
            print(f"Analytics update failed: {e}")
    
    # Run advanced rule-based analysis (fast, reliable)
    advanced_signals = {}
    try:
        from analysis_advanced import analyze_message_advanced
        advanced_result = analyze_message_advanced(state["raw_message"])
        advanced_signals = {
            "intent": advanced_result.primary_intent,
            "urgency": advanced_result.urgency_level,
            "sentiment": advanced_result.sentiment,
            "signals": advanced_result.intent_signals,
            "urgency_score": advanced_result.urgency_score
        }
        
        # Store metadata
        state["extracted_entities"] = {**advanced_result.entities}
        state["key_points"] = advanced_result.key_points
        state["action_items"] = advanced_result.action_items
        state["sentiment_score"] = advanced_result.sentiment_score
        state["frustration_level"] = advanced_result.frustration_level
    except Exception as e:
        print(f"Advanced analysis pre-pass failed: {e}")

    # Now use LLM for final classification, guided by rule-based signals
    history_block = ""
    if state.get("conversation_history"):
        history_block = f"\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n{state['conversation_history']}\n"
    
    hint_block = ""
    if advanced_signals:
        hint_block = f"\nØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ (ØªÙ„Ù…ÙŠØ­Ø§Øª):\n- Ø§Ù„Ù†ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©: {advanced_signals['intent']}\n- Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ù†ÙŠØ©: {', '.join(advanced_signals['signals'])}\n- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ø³ØªØ¹Ø¬Ø§Ù„: {advanced_signals['urgency']} (Ø¯Ø±Ø¬Ø©: {advanced_signals['urgency_score']})\n"

    prompt = f"""Ø­Ù„Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ÙˆØ­Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©.
    {hint_block}
    {history_block}
    
    Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ù†ÙŠØ© (intent): Ø§Ø³ØªÙØ³Ø§Ø±ØŒ Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©ØŒ Ø´ÙƒÙˆÙ‰ØŒ Ù…ØªØ§Ø¨Ø¹Ø©ØŒ Ø¹Ø±Ø¶ØŒ ØªØ³ÙˆÙŠÙ‚ØŒ Ø¢Ù„ÙŠØŒ Ø£Ø®Ø±Ù‰
    
    Ø§Ù„Ù†Øµ:
    {state['raw_message']}
    
    Ø£Ø±Ø¬Ø¹ JSON ÙÙ‚Ø·:
    {{
        "intent": "...", 
        "urgency": "Ø¹Ø§Ø¬Ù„/Ø¹Ø§Ø¯ÙŠ/Ù…Ù†Ø®ÙØ¶", 
        "sentiment": "Ø¥ÙŠØ¬Ø§Ø¨ÙŠ/Ù…Ø­Ø§ÙŠØ¯/Ø³Ù„Ø¨ÙŠ", 
        "language": "ar/en", 
        "dialect": "Ø´Ø§Ù…ÙŠ/Ø®Ù„ÙŠØ¬ÙŠ/Ù…ØµØ±ÙŠ/ÙØµØ­Ù‰/Ø£Ø®Ø±Ù‰",
        "reasoning": "Ø³Ø¨Ø¨ Ø§Ø®ØªÙŠØ§Ø± Ù‡Ø°Ø§ Ø§Ù„ØªØµÙ†ÙŠÙ"
    }}"""

    llm_response = await call_llm_enhanced(
        prompt, "Ø£Ù†Øª Ù…Ø­Ù„Ù„ Ù†ØµÙˆØµ Ø®Ø¨ÙŠØ± Ù„Ù†Ø¸Ø§Ù… Ø®Ø¯Ù…Ø© Ø¹Ù…Ù„Ø§Ø¡ Ø°ÙƒÙŠ.", temperature=0.1, json_mode=True
    )
    
    if llm_response:
        try:
            classification = json_repair.loads(llm_response.content)
            state["intent"] = classification.get("intent", advanced_signals.get("intent", "Ø£Ø®Ø±Ù‰"))
            state["urgency"] = classification.get("urgency", advanced_signals.get("urgency", "Ø¹Ø§Ø¯ÙŠ"))
            state["sentiment"] = classification.get("sentiment", advanced_signals.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯"))
            state["language"] = classification.get("language", "ar")
            state["dialect"] = classification.get("dialect", "ÙØµØ­Ù‰")
        except Exception as e:
            print(f"LLM Classification parsing failed: {e}")
            # Fallback to advanced labels if LLM fails
            if advanced_signals:
                state["intent"] = advanced_signals["intent"]
                state["urgency"] = advanced_signals["urgency"]
                state["sentiment"] = advanced_signals["sentiment"]
    elif advanced_signals:
        # Fallback to advanced labels
        state["intent"] = advanced_signals["intent"]
        state["urgency"] = advanced_signals["urgency"]
        state["sentiment"] = advanced_signals["sentiment"]
    
    # Customer relationship context
    if state.get("customer_history"):
        ch = state["customer_history"]
        order_count = ch.get("order_count", 0)
        if order_count > 5:
            state["relationship_level"] = "vip"
        elif order_count > 0:
            state["relationship_level"] = "returning"
        else:
            state["relationship_level"] = "new"
    
    # Auto-select persona based on intent/sentiment
    if not state.get("persona_name"):
        state["persona_name"] = get_persona_for_intent(
            state["intent"], state["sentiment"]
        )
        state["persona_auto_selected"] = True
    
    return state


async def enhanced_extract_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Extract with enhanced entity recognition"""
    state["processing_step"] = "Ø§Ø³ØªØ®Ø±Ø§Ø¬"
    
    # Regex-based extraction (reliable)
    message = state["raw_message"]
    entities = {}
    
    # Phone patterns
    phones = re.findall(r'(?:\+|00)?(?:963|966|971|962|961|20|965|974)\d{8,10}', message)
    if phones:
        entities["phones"] = list(set(phones))
    
    # Email
    emails = re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', message)
    if emails:
        entities["emails"] = emails
        state["sender_contact"] = emails[0]
    
    # Dates
    dates = re.findall(r'\d{1,2}[/\-]\d{1,2}[/\-]\d{2,4}', message)
    if dates:
        entities["dates"] = dates
    
    # Money
    amounts = re.findall(r'(\d+(?:,\d{3})*(?:\.\d+)?)\s*(?:Ù„\.Ø³|Ù„ÙŠØ±Ø©|Ø¯ÙˆÙ„Ø§Ø±|\$|Ø±\.Ø³)', message)
    if amounts:
        entities["amounts"] = amounts
    
    # Names
    name_match = re.search(r'(?:Ø§Ù„Ø³ÙŠØ¯|Ø§Ù„Ø³ÙŠØ¯Ø©|Ø§Ù„Ø£Ø³ØªØ§Ø°|Ø£Ø®ÙŠ|Ø£Ø®ØªÙŠ)\s+([\u0600-\u06FF\s]+)', message)
    if name_match:
        entities["mentioned_name"] = name_match.group(1).strip()
        if not state.get("sender_name"):
            state["sender_name"] = entities["mentioned_name"]
    
    state["extracted_entities"] = entities
    
    # LLM extraction for key points
    prompt = f"""Ø§Ø³ØªØ®Ø±Ø¬ Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ù„Ø©:
1. Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (3 ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
2. Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

Ø§Ù„Ø±Ø³Ø§Ù„Ø©:
{message}

JSON ÙÙ‚Ø·:
{{"key_points": ["..."], "action_items": ["..."]}}"""

    llm_response = await call_llm_enhanced(
        prompt,
        "Ø£Ù†Øª Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚.",
        temperature=0.2,
        json_mode=True
    )
    
    if llm_response:
        try:
            extracted = json_repair.loads(llm_response.content)
            state["key_points"] = extracted.get("key_points", [])
            state["action_items"] = extracted.get("action_items", [])
        except:
            state["key_points"] = []
            state["action_items"] = ["Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø±Ø³Ø§Ù„Ø©"]
    
    return state


async def enhanced_draft_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Generate human-like draft with persona, style learning, and anti-robotic patterns"""
    state["processing_step"] = "ØµÙŠØ§ØºØ©"
    
    persona_name = state.get("persona_name", "professional")
    persona = get_persona(persona_name)
    sender = state.get("sender_name", "Ø¹Ø²ÙŠØ²ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ„")
    intent = state.get("intent", "Ø£Ø®Ø±Ù‰")
    sentiment = state.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯")
    key_points = state.get("key_points", [])
    dialect = state.get("dialect", "ÙØµØ­Ù‰")
    
    # Build persona-aware system prompt
    system_prompt = build_persona_prompt(
        persona_name,
        state.get("preferences")
    )
    
    # Add learned style instructions if enabled
    style_instructions = ""
    if state.get("use_learned_style") and state.get("style_profile"):
        try:
            from style_learning import StyleProfile
            profile = StyleProfile.from_dict(state["style_profile"])
            style_instructions = f"""

=== Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù… Ù…Ù† Ø±Ø³Ø§Ø¦Ù„Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ===
{profile.to_prompt()}
===

Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ ÙÙŠ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø±Ø¯."""
        except Exception:
            pass
    
    # Get detected language
    language = state.get("language", "ar")
    
    # Get dynamic temperature
    temperature = get_dynamic_temperature(
        intent, sentiment, persona.temperature
    )
    
    # Build few-shot example
    few_shot = build_few_shot_prompt(intent)
    
    # Customer relationship context
    relationship_context = ""
    if state.get("relationship_level") == "vip":
        relationship_context = "\nThis is a VIP customer - show special appreciation."
    elif state.get("relationship_level") == "returning":
        relationship_context = "\nThis is a returning customer - you can acknowledge that."
    
    # Knowledge and tools context
    knowledge_block = ""
    if state.get("knowledge_facts"):
        facts = "\n".join(state["knowledge_facts"])
        knowledge_block = f"\n=== KNOWLEDGE BASE FACTS (USE THESE TO BE ACCURATE) ===\n{facts}\n"
        
    tool_block = ""
    if state.get("tool_calls"):
        tools = json.dumps(state["tool_calls"], ensure_ascii=False)
        tool_block = f"\n=== LIVE TOOL RESULTS ===\n{tools}\n"
    
    # Build language-specific prompt
    if language and language != "ar":
        # Non-Arabic language - respond in same language
        language_names = {
            "en": "English",
            "fr": "French", 
            "es": "Spanish",
            "de": "German",
            "tr": "Turkish",
        }
        lang_name = language_names.get(language, language.upper())
        
        history_block = ""
        if state.get("conversation_history"):
            history_block = f"\nPREVIOUS CONVERSATION CONTEXT:\n{state['conversation_history']}\n"

        prompt = f"""{few_shot}

ğŸ—£ï¸ IMPORTANT: Respond in {lang_name} (same language as customer)!

{history_block}

Write a response to the customer ({sender}) based on:
- Message type: {intent}
- Sentiment: {sentiment}
- Language: {lang_name}
- Key points: {', '.join(key_points) or 'Not specified'}
{relationship_context}
{style_instructions}
{knowledge_block}
{tool_block}

Customer's message:
{state['raw_message']}

âš ï¸ Very important: Match the customer's language! Respond in {lang_name}.

Write only the response in {lang_name} (3-6 lines), no explanation:"""

    else:
        # Arabic - handle dialects
        dialect_instruction = ""
        if dialect and dialect != "ÙØµØ­Ù‰":
            dialect_examples = {
                "Ø³Ø¹ÙˆØ¯ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©/Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'ÙˆØ´ ØªØ­ØªØ§Ø¬ØŸ'ØŒ 'ØªÙ…Ø§Ù…'ØŒ 'Ø¥Ù† Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡'ØŒ 'ÙŠØ¹Ø·ÙŠÙƒ Ø§Ù„Ø¹Ø§ÙÙŠØ©'ØŒ 'ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ'",
                "Ø®Ù„ÙŠØ¬ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø´Ù„ÙˆÙ†ÙƒØŸ'ØŒ 'Ø²ÙŠÙ†'ØŒ 'ÙˆØ§Ø¬Ø¯'ØŒ 'ÙŠØ§ Ù‡Ù„Ø§'ØŒ 'ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø®Ø¯Ù…ÙƒØŸ'",
                "Ù…ØµØ±ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø¥Ø²ÙŠÙƒØŸ'ØŒ 'ØªÙ…Ø§Ù…'ØŒ 'Ø¹Ø§ÙŠØ² Ø¥ÙŠÙ‡ØŸ'ØŒ 'Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø¥Ø²Ø§ÙŠØŸ'ØŒ 'Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©'",
                "Ø´Ø§Ù…ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø´Ø§Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'ÙƒÙŠÙÙƒØŸ'ØŒ 'Ø´Ùˆ Ø¨Ø¯ÙƒØŸ'ØŒ 'Ù…Ù†ÙŠØ­'ØŒ 'Ù‡Ù„Ù‚'ØŒ 'ÙƒØªÙŠØ± Ù…Ù†ÙŠØ­'",
                "Ø³ÙˆØ±ÙŠ": "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø³ÙˆØ±ÙŠØ© ÙÙŠ Ø§Ù„Ø±Ø¯. Ù…Ø«Ø§Ù„: 'Ø´Ùˆ Ø¨Ø¯ÙƒØŸ'ØŒ 'ÙƒÙŠÙÙƒØŸ'ØŒ 'Ù…Ù†ÙŠØ­'ØŒ 'Ù‡Ù„Ù‚'ØŒ 'Ù„ÙŠÙƒ'",
            }
            dialect_instruction = dialect_examples.get(dialect, f"Ø§Ø³ØªØ®Ø¯Ù… Ù„Ù‡Ø¬Ø© {dialect} ÙÙŠ Ø§Ù„Ø±Ø¯ Ø¥Ù† Ø£Ù…ÙƒÙ†.")
        
        # Anti-robotic instructions
        anti_robotic = f"""
ØªØ¬Ù†Ø¨ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ©:
{', '.join(ROBOTIC_PHRASES[:5])}

Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ¹ÙÙˆÙŠØ©."""
        
        history_block = ""
        if state.get("conversation_history"):
            history_block = f"\nØ³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n{state['conversation_history']}\n"

        prompt = f"""{few_shot}

ğŸ—£ï¸ Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: {dialect}
{dialect_instruction if dialect_instruction else "Ø§Ø³ØªØ®Ø¯Ù… Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ Ù…Ø¨Ø³Ù‘Ø·Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…."}

{history_block}

Ø§ÙƒØªØ¨ Ø±Ø¯Ø§Ù‹ Ù„Ù„Ø¹Ù…ÙŠÙ„ ({sender}) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰:
- Ù†ÙˆØ¹ Ø§Ù„Ø±Ø³Ø§Ù„Ø©: {intent}
- Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {sentiment}
- Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {dialect}
- Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {', '.join(key_points) or 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯Ø©'}
{relationship_context}
{style_instructions}
{knowledge_block.replace('KNOWLEDGE BASE FACTS', 'Ø­Ù‚Ø§Ø¦Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©').replace('LIVE TOOL RESULTS', 'Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©')}
{tool_block.replace('LIVE TOOL RESULTS', 'Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©')}

Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„:
{state['raw_message']}
{anti_robotic}

âš ï¸ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹: Ø·Ø§Ø¨Ù‚ Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙÙŠ Ø±Ø¯Ùƒ! Ø¥Ø°Ø§ ÙƒØªØ¨ Ø¨Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØŒ Ø±Ø¯ Ø¨Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ. Ø¥Ø°Ø§ ÙƒØªØ¨ Ø¨Ø§Ù„Ù…ØµØ±ÙŠØŒ Ø±Ø¯ Ø¨Ø§Ù„Ù…ØµØ±ÙŠ.

Ø§ÙƒØªØ¨ Ø§Ù„Ø±Ø¯ ÙÙ‚Ø· Ø¨Ù†ÙØ³ Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ (3-6 Ø£Ø³Ø·Ø±)ØŒ Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­:"""

    llm_response = await call_llm_enhanced(
        prompt,
        system_prompt,
        temperature=temperature,
        max_tokens=400,
    )
    
    if llm_response and llm_response.content and len(llm_response.content) > 40:
        # Post-process: remove any remaining robotic phrases
        draft = remove_robotic_phrases(llm_response.content)
        state["draft_response"] = draft
    else:
        # Fallback with persona-aware greeting/closing
        greeting = get_random_greeting(persona_name, sender)
        closing = get_random_closing(persona_name)
        
        state["draft_response"] = f"""{greeting}

ÙˆØµÙ„ØªÙ†ÙŠ Ø±Ø³Ø§Ù„ØªÙƒ ÙˆØ³Ø£ØªØ§Ø¨Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.

{closing}"""
    
    # Generate summary
    state["summary"] = f"Ø±Ø³Ø§Ù„Ø© {intent} Ù…Ù† {sender}. Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: {sentiment}. Ø§Ù„Ù„Ù‡Ø¬Ø©: {dialect}."
    
    # Check response quality
    quality = check_response_quality(state["draft_response"])
    state["response_quality_score"] = quality["score"]
    state["response_quality_issues"] = quality["issues"]
    
    # Suggested actions
    actions_map = {
        "Ø§Ø³ØªÙØ³Ø§Ø±": ["Ø§Ù„Ø±Ø¯", "Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"],
        "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©": ["Ø¥Ù†Ø´Ø§Ø¡ Ø·Ù„Ø¨", "ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¹Ø¯"],
        "Ø´ÙƒÙˆÙ‰": ["ØªØµØ¹ÙŠØ¯", "ÙØªØ­ ØªØ°ÙƒØ±Ø©", "Ø§ØªØµØ§Ù„"],
        "Ù…ØªØ§Ø¨Ø¹Ø©": ["ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø­Ø§Ù„Ø©"],
        "Ø¹Ø±Ø¶": ["Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ø¹Ø±Ø¶"],
        "Ø£Ø®Ø±Ù‰": ["Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©"],
    }
    state["suggested_actions"] = actions_map.get(intent, ["Ù…Ø±Ø§Ø¬Ø¹Ø©"])
    
    # Update analytics (replied)
    if state.get("preferences") and state["preferences"].get("license_key_id"):
        try:
            asyncio.create_task(update_daily_analytics(
                license_id=state["preferences"]["license_key_id"],
                messages_replied=1,
                sentiment=state.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯"),
                time_saved_seconds=180 # 3 minutes per AI response
            ))
        except Exception as e:
            print(f"Analytics reply update failed: {e}")

    return state


async def enhanced_verify_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Actor-Critic Node: Verify the draft response for hallucinations or quality issues"""
    state["processing_step"] = "ØªØ¯Ù‚ÙŠÙ‚"
    
    draft = state.get("draft_response", "")
    if not draft or draft.startswith("â³"):
        return state
        
    entities = state.get("extracted_entities", {})
    preferences = state.get("preferences", {})
    
    # Context for the critic
    context = f"""
    FACTS EXTRACTED FROM MESSAGE:
    {json.dumps(entities, ensure_ascii=False)}
    
    BUSINESS PREFERENCES:
    {json.dumps(preferences, ensure_ascii=False)}
    
    DRAFT RESPONSE TO VERIFY:
    {draft}
    """
    
    prompt = f"""You are an AI Critic. Your job is to ensure the customer response is accurate, professional, and free of hallucinations.
    
    Compare the DRAFT RESPONSE with the FACTS and BUSINESS PREFERENCES.
    
    Check for:
    1. HALLUCINATIONS: Does the response mention prices, dates, or facts not in the context?
    2. UNPROFESSIONAL TONE: Is it too robotic or rude?
    3. MISSING INFO: Did the customer ask something that was ignored?
    
    Output JSON ONLY:
    {{
        "is_valid": true/false,
        "score": 0-100,
        "reason": "Explain why it failed if invalid",
        "critic_feedback": "Instructions for the AI to fix the response if invalid"
    }}
    
    {context}
    """
    
    llm_response = await call_llm_enhanced(
        prompt,
        "You are a strict quality control auditor for Arabic customer service.",
        temperature=0.1,
        json_mode=True
    )
    
    if llm_response:
        try:
            verification = json_repair.loads(llm_response.content)
            state["response_quality_score"] = verification.get("score", state["response_quality_score"])
            
            if not verification.get("is_valid", True) and verification.get("score", 100) < 70:
                state["response_quality_issues"].append(verification.get("reason", "Hallucination detected"))
                state["error"] = f"Verification failed: {verification.get('reason')}"
                # Add feedback for regeneration
                state["summary"] += f" (Verification failed: {verification.get('reason')})"
                print(f"Critic rejected response: {verification.get('reason')}")
            else:
                state["error"] = None
        except Exception as e:
            print(f"Verification parsing failed: {e}")
            
    return state

async def retrieve_knowledge_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Step 2b: Retrieve relevant business facts (RAG)"""
    state["processing_step"] = "Ø¨Ø­Ø« Ø§Ù„Ù…Ø¹Ø±ÙØ©"
    
    try:
        kb = get_knowledge_base()
        query = state["raw_message"]
        
        # Search Knowledge Base
        results = await kb.search(query, k=3)
        
        # Filter and store high-quality facts
        facts = []
        for res in results:
            if res.get("score", 1.0) < 0.5: # Lower distance = better match
                facts.append(res["text"])
        
        state["knowledge_facts"] = facts
        if facts:
            print(f"RAG: Found {len(facts)} relevant facts.")
        else:
            state["knowledge_facts"] = []
        
    except Exception as e:
        print(f"Knowledge retrieval error: {e}")
        state["knowledge_facts"] = []
        
    return state


async def tool_node(state: EnhancedAgentState) -> EnhancedAgentState:
    """Step 2c: Execute actionable tools if needed"""
    state["processing_step"] = "Ø£Ø¯ÙˆØ§Øª"
    
    intent = state.get("intent", "")
    if intent not in ["Ø§Ø³ØªÙØ³Ø§Ø±", "Ø·Ù„Ø¨", "Ø·Ù„Ø¨ Ø®Ø¯Ù…Ø©", "info", "order"]:
        return state

    try:
        from tools.business_tools import BUSINESS_TOOLS, execute_tool
        
        # Call LLM specifically for tools
        prompt = f"Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙŠØ³Ø£Ù„: {state['raw_message']}\n\nØ­Ø¯Ø¯ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø£ÙŠØ© Ø£Ø¯ÙˆØ§Øª ØªØ­ØªØ§Ø¬ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¦Ù‡Ø§ Ù„ØªÙˆÙÙŠØ± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©."
        response = await call_llm_enhanced(
            prompt, 
            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ù…ÙÙˆØ¶ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙ‚Ø·.",
            tools=BUSINESS_TOOLS,
            temperature=0
        )
        
        # Check for tool calls in response
        if response and hasattr(response, 'tool_calls') and response.tool_calls:
            results = []
            for tc in response.tool_calls:
                print(f"Executing Tool: {tc.name} with {tc.args}")
                tool_res = await execute_tool(tc.name, tc.args)
                results.append({
                    "tool": tc.name,
                    "result": tool_res
                })
            state["tool_calls"] = results
            print(f"Tool execution results: {len(results)} success.")
        
    except Exception as e:
        print(f"Tool execution error: {e}")
        
    return state


# ============ Build Enhanced Graph ============

def create_enhanced_agent():
    """Create the enhanced InboxCRM agent"""
    workflow = StateGraph(EnhancedAgentState)
    
    workflow.add_node("classify", enhanced_classify_node)
    workflow.add_node("extract", enhanced_extract_node)
    workflow.add_node("retrieve", retrieve_knowledge_node)
    workflow.add_node("tool", tool_node)
    workflow.add_node("draft", enhanced_draft_node)
    workflow.add_node("verify", enhanced_verify_node)
    
    # Routing logic
    def route_enhanced(state: EnhancedAgentState):
        intent = state.get("intent", "Ø£Ø®Ø±Ù‰")
        if intent in ["ØªØ³ÙˆÙŠÙ‚", "Ø¢Ù„ÙŠ", "spam", "marketing", "automated"]:
            return "end"
        return "extract"

    def route_after_verify(state: EnhancedAgentState):
        """Loop back if quality is too low, or flag for human intervention"""
        
        # Sentiment-based Escalation Logic
        sentiment = state.get("sentiment", "Ù…Ø­Ø§ÙŠØ¯")
        urgency = state.get("urgency", "Ø¹Ø§Ø¯ÙŠ")
        quality_score = state.get("response_quality_score", 100)
        
        # Market-Ready Rule: Escalate if angry and urgent, or if quality is consistently low
        if (sentiment == "Ø³Ù„Ø¨ÙŠ" and urgency == "Ø¹Ø§Ø¬Ù„") or quality_score < 60:
            state["needs_human_intervention"] = True
            print(f"MARKET-READY: Escalating to human (Sentiment: {sentiment}, Quality: {quality_score})")
            
        if state.get("error") and "Verification failed" in state["error"]:
             # In production we'd use a loop counter via state
             return "end" 
        return "end"

    workflow.set_entry_point("classify")
    workflow.add_conditional_edges(
        "classify",
        route_enhanced,
        {
            "extract": "extract",
            "end": END
        }
    )
    workflow.add_edge("extract", "retrieve")
    workflow.add_edge("retrieve", "tool")
    workflow.add_edge("tool", "draft")
    workflow.add_edge("draft", "verify")
    workflow.add_conditional_edges(
        "verify",
        route_after_verify,
        {
            "end": END
        }
    )
    
    return workflow.compile()


# Singleton
_enhanced_agent = None


def get_enhanced_agent():
    """Get or create the enhanced agent instance"""
    global _enhanced_agent
    if _enhanced_agent is None:
        _enhanced_agent = create_enhanced_agent()
    return _enhanced_agent


async def process_message_enhanced(
    message: str,
    message_type: str = None,
    sender_name: str = None,
    sender_contact: str = None,
    preferences: Optional[Dict[str, Any]] = None,
    conversation_history: Optional[str] = None,
    customer_history: Optional[Dict[str, Any]] = None,
    persona_name: str = None,
    # Style learning options
    use_learned_style: bool = False,
    style_profile: Optional[Dict[str, Any]] = None,
) -> dict:
    """
    Process a message with enhanced human-like responses.
    
    Args:
        message: The raw message text
        message_type: Type of message (email, telegram, whatsapp, general)
        sender_name: Customer's name
        sender_contact: Customer's email or phone
        preferences: Business preferences (tone, business_name, etc.)
        conversation_history: Previous conversation with this customer
        customer_history: Customer data (order_count, etc.)
        persona_name: Specific persona to use (professional, friendly, etc.)
        use_learned_style: If True, use learned style from user's past messages
        style_profile: The StyleProfile dict (from analyze_messages_for_style)
    """
    
    agent = get_enhanced_agent()
    
    # --- Step 0: Local Blocking (Smart Filtering) ---
    should_process, reason = await apply_filters(
        message={"body": message, "sender_contact": sender_contact},
        license_id=preferences.get("license_key_id", 0) if preferences else 0,
        recent_messages=None
    )
    
    if not should_process:
        print(f"Enhanced Agent: Message filtered locally: {reason}")
        return {
            "success": True,
            "data": {
                "intent": "Ø¢Ù„ÙŠ" if "Automated" in reason else "ignored",
                "urgency": "Ù…Ù†Ø®ÙØ¶",
                "sentiment": "Ù…Ø­Ø§ÙŠØ¯",
                "summary": f"ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø©: {reason}",
                "draft_response": "", 
                "processing_notes": f"Filtered by: {reason}",
                # Fill required enhanced fields with dummies
                "persona_used": "none",
                "persona_auto_selected": False,
                "relationship_level": "new",
                "key_points": [],
                "action_items": [],
                "extracted_entities": {},
                "suggested_actions": [],
                "message_type": message_type or "general",
                "language": "ar",
                "dialect": None,
                "sender_name": sender_name,
                "sender_contact": sender_contact,
                "quality_score": 0,
                "quality_issues": []
            }
        }

    initial_state: EnhancedAgentState = {

        "raw_message": message,
        "message_type": message_type or "general",
        "intent": "",
        "urgency": "",
        "sentiment": "",
        "language": None,
        "dialect": None,
        "sender_name": sender_name,
        "sender_contact": sender_contact,
        "key_points": [],
        "action_items": [],
        "extracted_entities": {},
        "customer_history": customer_history,
        "relationship_level": "new",
        "persona_name": persona_name or "",
        "persona_auto_selected": False,
        "use_learned_style": use_learned_style,
        "style_profile": style_profile,
        "summary": "",
        "draft_response": "",
        "suggested_actions": [],
        "response_quality_score": 0,
        "response_quality_issues": [],
        "error": None,
        "processing_step": "",
        "preferences": preferences,
        "conversation_history": conversation_history,
        "knowledge_facts": [],
        "tool_calls": [],
        "needs_human_intervention": False,
    }
    
    try:
        final_state = await agent.ainvoke(initial_state)
        return {
            "success": True,
            "data": {
                "intent": final_state["intent"],
                "urgency": final_state["urgency"],
                "sentiment": final_state["sentiment"],
                "language": final_state.get("language"),
                "dialect": final_state.get("dialect"),
                "sender_name": final_state["sender_name"],
                "sender_contact": final_state["sender_contact"],
                "key_points": final_state["key_points"],
                "action_items": final_state["action_items"],
                "extracted_entities": final_state["extracted_entities"],
                "summary": final_state["summary"],
                "draft_response": final_state["draft_response"],
                "suggested_actions": final_state["suggested_actions"],
                "message_type": final_state["message_type"],
                # New fields
                "persona_used": final_state["persona_name"],
                "persona_auto_selected": final_state["persona_auto_selected"],
                "relationship_level": final_state["relationship_level"],
                "quality_score": final_state["response_quality_score"],
                "quality_issues": final_state["response_quality_issues"],
            }
        }
    except Exception as e:
        return {
            "success": False,
            "error": f"Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}"
        }
